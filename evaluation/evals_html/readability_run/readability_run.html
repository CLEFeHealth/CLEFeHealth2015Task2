<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2015 TASK 2 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2015 Task 2 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2015 TASK 2 Results - readability_run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2015 TASK 2 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the two highest priority runs (run 2 and 3); thus all remaining runs were not sampled to form the assessment pool. This was because, unlike in previous years, submissions were greatly differing between each other, and thus lead to very large (and different) pools. As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page. Note that metrics such as MAP may not be reliable for evaluation due to the limited pool depth.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtest2015.XX id); others did not retrieve results for some queries or fail to conform to the TREC result format; finally, some retrieved the same document more than once for the same query (i.e. duplicate documents). To fairly evaluate the runs of every participant, submitted runs containing formatting errors and duplicated documents have been corrected. Duplicate documents have been removed and replaced by a <i>duplicatedid</i> string. Your runs (as evaluated here) are contained in the folder <i>runs</i> - please use these runs if you want to perform further evaluation and analysis.</br></br>Please refer to the <a href="https://sites.google.com/site/clefehealth2015/">ShARe/CLEF 2015 eHealth Evaluation Lab website</a> for additional details on the task.</p></br><p>Relevance assessments are distributed along with this webpage and are also available from the task website. Assessors judged relevance according to a three point scale: Not Relevant (label 0), Somewhat Relevant (label 1), Highly Relevant (label 2). When computing binary relevance measures (e.g. P@10 and MAP), we mapped label 0 to not relevant, and labels 1 and 2 to relevant; this  is encoded in the binary qrels named <i>qrels.clef2015.test.bin.txt</i>. Graded relevance assessments are contained in the <i>qrels.clef2015.test.graded.txt</i> file.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2015.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2015.test.bin.txt runName</pre>
 <h4>readability_run.1.dat</h4>
            <pre>
runid                 	all	Xapian_BM25_bs4_dale_alpha
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1594
map                   	all	0.2138
gm_map                	all	0.0648
Rprec                 	all	0.2421
bpref                 	all	0.2866
recip_rank            	all	0.5746
iprec_at_recall_0.00  	all	0.6124
iprec_at_recall_0.10  	all	0.4725
iprec_at_recall_0.20  	all	0.3572
iprec_at_recall_0.30  	all	0.3064
iprec_at_recall_0.40  	all	0.2576
iprec_at_recall_0.50  	all	0.1997
iprec_at_recall_0.60  	all	0.1358
iprec_at_recall_0.70  	all	0.0982
iprec_at_recall_0.80  	all	0.0709
iprec_at_recall_0.90  	all	0.0323
iprec_at_recall_1.00  	all	0.0189
P_5                   	all	0.4000
P_10                  	all	0.3424
P_15                  	all	0.3111
P_20                  	all	0.2894
P_30                  	all	0.2460
P_100                 	all	0.1362
P_200                 	all	0.0881
P_500                 	all	0.0436
P_1000                	all	0.0242
</pre>
 <h4>readability_run.2.dat</h4>
            <pre>
runid                 	all	Xapian_BM25_bs4_kiss_direct
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1594
map                   	all	0.2100
gm_map                	all	0.0630
Rprec                 	all	0.2293
bpref                 	all	0.2943
recip_rank            	all	0.5466
iprec_at_recall_0.00  	all	0.5987
iprec_at_recall_0.10  	all	0.4998
iprec_at_recall_0.20  	all	0.3661
iprec_at_recall_0.30  	all	0.3127
iprec_at_recall_0.40  	all	0.2356
iprec_at_recall_0.50  	all	0.1762
iprec_at_recall_0.60  	all	0.1314
iprec_at_recall_0.70  	all	0.0952
iprec_at_recall_0.80  	all	0.0651
iprec_at_recall_0.90  	all	0.0255
iprec_at_recall_1.00  	all	0.0148
P_5                   	all	0.4061
P_10                  	all	0.3606
P_15                  	all	0.3192
P_20                  	all	0.2803
P_30                  	all	0.2409
P_100                 	all	0.1291
P_200                 	all	0.0830
P_500                 	all	0.0429
P_1000                	all	0.0242
</pre>
 <h4>readability_run.3.dat</h4>
            <pre>
runid                 	all	Xapian_BM25_bs4_dale_log
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1594
map                   	all	0.1835
gm_map                	all	0.0539
Rprec                 	all	0.2042
bpref                 	all	0.2743
recip_rank            	all	0.4956
iprec_at_recall_0.00  	all	0.5514
iprec_at_recall_0.10  	all	0.4535
iprec_at_recall_0.20  	all	0.3293
iprec_at_recall_0.30  	all	0.2662
iprec_at_recall_0.40  	all	0.1940
iprec_at_recall_0.50  	all	0.1494
iprec_at_recall_0.60  	all	0.1113
iprec_at_recall_0.70  	all	0.0746
iprec_at_recall_0.80  	all	0.0536
iprec_at_recall_0.90  	all	0.0250
iprec_at_recall_1.00  	all	0.0136
P_5                   	all	0.3667
P_10                  	all	0.3364
P_15                  	all	0.2838
P_20                  	all	0.2568
P_30                  	all	0.2207
P_100                 	all	0.1179
P_200                 	all	0.0767
P_500                 	all	0.0418
P_1000                	all	0.0242
</pre>
 <h4>readability_run.4.dat</h4>
            <pre>
runid                 	all	Xapian_BM25_bs4_flesh_log
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1594
map                   	all	0.0847
gm_map                	all	0.0258
Rprec                 	all	0.1050
bpref                 	all	0.2373
recip_rank            	all	0.1789
iprec_at_recall_0.00  	all	0.2401
iprec_at_recall_0.10  	all	0.1879
iprec_at_recall_0.20  	all	0.1509
iprec_at_recall_0.30  	all	0.1281
iprec_at_recall_0.40  	all	0.1019
iprec_at_recall_0.50  	all	0.0848
iprec_at_recall_0.60  	all	0.0652
iprec_at_recall_0.70  	all	0.0506
iprec_at_recall_0.80  	all	0.0334
iprec_at_recall_0.90  	all	0.0140
iprec_at_recall_1.00  	all	0.0063
P_5                   	all	0.1121
P_10                  	all	0.1227
P_15                  	all	0.1232
P_20                  	all	0.1197
P_30                  	all	0.1187
P_100                 	all	0.0792
P_200                 	all	0.0561
P_500                 	all	0.0355
P_1000                	all	0.0242
</pre>
 <h4>readability_run.5.dat</h4>
            <pre>
runid                 	all	lucene_vsm_html_kiss_direct
num_q                 	all	66
num_ret               	all	65689
num_rel               	all	1972
num_rel_ret           	all	1519
map                   	all	0.1532
gm_map                	all	0.0520
Rprec                 	all	0.1803
bpref                 	all	0.2604
recip_rank            	all	0.4618
iprec_at_recall_0.00  	all	0.5174
iprec_at_recall_0.10  	all	0.4002
iprec_at_recall_0.20  	all	0.2960
iprec_at_recall_0.30  	all	0.2070
iprec_at_recall_0.40  	all	0.1570
iprec_at_recall_0.50  	all	0.1190
iprec_at_recall_0.60  	all	0.0901
iprec_at_recall_0.70  	all	0.0682
iprec_at_recall_0.80  	all	0.0351
iprec_at_recall_0.90  	all	0.0138
iprec_at_recall_1.00  	all	0.0052
P_5                   	all	0.3636
P_10                  	all	0.3076
P_15                  	all	0.2747
P_20                  	all	0.2424
P_30                  	all	0.2056
P_100                 	all	0.1077
P_200                 	all	0.0692
P_500                 	all	0.0382
P_1000                	all	0.0230
</pre>
 <h4>readability_run.6.dat</h4>
            <pre>
runid                 	all	lucene_vsm_html_dale_log
num_q                 	all	66
num_ret               	all	65689
num_rel               	all	1972
num_rel_ret           	all	1519
map                   	all	0.1464
gm_map                	all	0.0498
Rprec                 	all	0.1838
bpref                 	all	0.2537
recip_rank            	all	0.4280
iprec_at_recall_0.00  	all	0.4924
iprec_at_recall_0.10  	all	0.3848
iprec_at_recall_0.20  	all	0.2773
iprec_at_recall_0.30  	all	0.2080
iprec_at_recall_0.40  	all	0.1594
iprec_at_recall_0.50  	all	0.1223
iprec_at_recall_0.60  	all	0.0930
iprec_at_recall_0.70  	all	0.0591
iprec_at_recall_0.80  	all	0.0354
iprec_at_recall_0.90  	all	0.0139
iprec_at_recall_1.00  	all	0.0052
P_5                   	all	0.3485
P_10                  	all	0.2970
P_15                  	all	0.2566
P_20                  	all	0.2288
P_30                  	all	0.1980
P_100                 	all	0.1067
P_200                 	all	0.0698
P_500                 	all	0.0380
P_1000                	all	0.0230
</pre>
 <h4>readability_run.7.dat</h4>
            <pre>
runid                 	all	lucene_vsm_html_flesh_log
num_q                 	all	66
num_ret               	all	65689
num_rel               	all	1972
num_rel_ret           	all	1519
map                   	all	0.1312
gm_map                	all	0.0434
Rprec                 	all	0.1584
bpref                 	all	0.2379
recip_rank            	all	0.3368
iprec_at_recall_0.00  	all	0.4109
iprec_at_recall_0.10  	all	0.2975
iprec_at_recall_0.20  	all	0.2381
iprec_at_recall_0.30  	all	0.2054
iprec_at_recall_0.40  	all	0.1567
iprec_at_recall_0.50  	all	0.1137
iprec_at_recall_0.60  	all	0.0880
iprec_at_recall_0.70  	all	0.0685
iprec_at_recall_0.80  	all	0.0379
iprec_at_recall_0.90  	all	0.0140
iprec_at_recall_1.00  	all	0.0049
P_5                   	all	0.2424
P_10                  	all	0.2288
P_15                  	all	0.2182
P_20                  	all	0.1992
P_30                  	all	0.1778
P_100                 	all	0.1079
P_200                 	all	0.0711
P_500                 	all	0.0380
P_1000                	all	0.0230
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2015.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2015.test.graded.txt runName</pre></br><p>This computes nDCG according to Jarvelin and Kekalainen (ACM ToIS v. 20, pp. 422-446, 2002). Gain values are the relevance values in the qrels file (i.e. label 0 corresponds to gain 0, label 1 to gain 1 and label 2 to gain 2).</p>
 <h4>readability_run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3378
ndcg_cut_10           	all	0.3226
ndcg_cut_15           	all	0.3145
ndcg_cut_20           	all	0.3101
ndcg_cut_30           	all	0.3052
ndcg_cut_100          	all	0.3421
ndcg_cut_200          	all	0.3913
ndcg_cut_500          	all	0.4309
ndcg_cut_1000         	all	0.4548
</pre>
 <h4>readability_run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3370
ndcg_cut_10           	all	0.3299
ndcg_cut_15           	all	0.3193
ndcg_cut_20           	all	0.3076
ndcg_cut_30           	all	0.3059
ndcg_cut_100          	all	0.3366
ndcg_cut_200          	all	0.3794
ndcg_cut_500          	all	0.4275
ndcg_cut_1000         	all	0.4530
</pre>
 <h4>readability_run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2815
ndcg_cut_10           	all	0.2890
ndcg_cut_15           	all	0.2747
ndcg_cut_20           	all	0.2705
ndcg_cut_30           	all	0.2717
ndcg_cut_100          	all	0.3009
ndcg_cut_200          	all	0.3420
ndcg_cut_500          	all	0.3981
ndcg_cut_1000         	all	0.4285
</pre>
 <h4>readability_run.4.dat</h4>
            <pre>
ndcg_cut_5            	all	0.0802
ndcg_cut_10           	all	0.0958
ndcg_cut_15           	all	0.1054
ndcg_cut_20           	all	0.1091
ndcg_cut_30           	all	0.1202
ndcg_cut_100          	all	0.1698
ndcg_cut_200          	all	0.2079
ndcg_cut_500          	all	0.2681
ndcg_cut_1000         	all	0.3287
</pre>
 <h4>readability_run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2639
ndcg_cut_10           	all	0.2595
ndcg_cut_15           	all	0.2566
ndcg_cut_20           	all	0.2488
ndcg_cut_30           	all	0.2440
ndcg_cut_100          	all	0.2775
ndcg_cut_200          	all	0.3143
ndcg_cut_500          	all	0.3651
ndcg_cut_1000         	all	0.3985
</pre>
 <h4>readability_run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2450
ndcg_cut_10           	all	0.2456
ndcg_cut_15           	all	0.2376
ndcg_cut_20           	all	0.2328
ndcg_cut_30           	all	0.2322
ndcg_cut_100          	all	0.2691
ndcg_cut_200          	all	0.3085
ndcg_cut_500          	all	0.3567
ndcg_cut_1000         	all	0.3914
</pre>
 <h4>readability_run.7.dat</h4>
            <pre>
ndcg_cut_5            	all	0.1696
ndcg_cut_10           	all	0.1834
ndcg_cut_15           	all	0.1916
ndcg_cut_20           	all	0.1903
ndcg_cut_30           	all	0.1967
ndcg_cut_100          	all	0.2477
ndcg_cut_200          	all	0.2893
ndcg_cut_500          	all	0.3373
ndcg_cut_1000         	all	0.3713
</pre>

<h2>Readability-biased evaluation</h2>
        <p>For this year task, we have considered the factor of understandability of information (or readability) within the evaluation of the submissions, along with the topicality factor (normally referred to as (topical) relevance). Thus, along with (topical) relevance assessments (qrels), we have collected readability assessments (qread). These assessments were provided by judges along with the relevance assessments; however readability was assumed to be assessed independently of (topical) relevance. To account for understandability/readability in the evaluation, we have computed an understandability biased measure, uRBP, as defined in Zuccon&Koopman, <a href="http://zuccon.net/publications/medIR2014_healthsearch_readability.pdf">"Integrating understandability in the evaluation of consumer health search engines"</a>, MedIR 2014. We refer to that publication for the motivations and the details of the measure; note however that we did not use automated readability measures to estimate readability - we instead had actual readability assessments from the relevance assessors. Readability assessments were given on a 4 point scale (from 0 to 3): It is very technical and difficult to read and understand (label 0), It is somewhat technical and difficult to read and understand (label 1), It is somewhat easy to read and understand (label 2), It is very easy to read and understand (label 3).</p><p>The results below have been obtained with the binary relevance assessments (i.e. qrels.clef2015.test.bin.txt) and the graded readability assessments (i.e. i.e. qread.clef2015.test.graded.txt), and ubire-v0.1.0 as distributed on <a href="https://github.com/ielab/ubire">GitHub</a>.</br>The tool was ran as follows:</p>
<pre>java -jar /tools/ubire.0.1.jar --qrels-file=qrels/qrels.clef2015.test.bin.txt --qread-file=qrels/qread.clef2015.test.graded.txt --readability --rbp-p=0.8 --ranking-file=runName</pre></br><p>This computes RBP, and two versions of uRBP. The user persistence parameter <i>p</i> of RBP (and uRBP) was set to 0.8 following Park&Zhang, <a href="http://goanna.cs.rmit.edu.au/~aht/adcs2007/papers/17N.PDF">"On the distribution of user persistence for rank-biased precision"</a>, ADCS 2007.  uRBP has been computed by using user model 1 of Zuccon&Koopman with threshold=2, i.e. documents with a readability score of 0 or 1 where deemed not readable and thus had P(U|k)=0, while documents with a readability score of 2 or 3 where deemed readable and thus had P(U|k)=1. uRBPgr has been computed by mapping graded readability scores to different probability values, in particular: readability of 0 was assigned P(U|k)=0, readability of 1 was assigned P(U|k)=0.4, readability of 2 was assigned P(U|k)=0.8, readability of 3 was assigned P(U|k)=1.</br>Note that we are still experimenting with these readability-biased measures and thus observations made with the provided measures may not be conclusive.</p>
 <h4>readability_run.1.dat</h4>
            <pre>
RBP(0.8)	all      	0.3675
uRBP(0.8)	all      	0.3140
uRBPgr(0.8)	all      	0.3064
</pre>
 <h4>readability_run.2.dat</h4>
            <pre>
RBP(0.8)	all      	0.3756
uRBP(0.8)	all      	0.3154
uRBPgr(0.8)	all      	0.3117
</pre>
 <h4>readability_run.3.dat</h4>
            <pre>
RBP(0.8)	all      	0.3390
uRBP(0.8)	all      	0.3067
uRBPgr(0.8)	all      	0.2929
</pre>
 <h4>readability_run.4.dat</h4>
            <pre>
RBP(0.8)	all      	0.1143
uRBP(0.8)	all      	0.1080
uRBPgr(0.8)	all      	0.1000
</pre>
 <h4>readability_run.5.dat</h4>
            <pre>
RBP(0.8)	all      	0.0362
uRBP(0.8)	all      	0.0160
uRBPgr(0.8)	all      	0.0227
</pre>
 <h4>readability_run.6.dat</h4>
            <pre>
RBP(0.8)	all      	0.0194
uRBP(0.8)	all      	0.0117
uRBPgr(0.8)	all      	0.0134
</pre>
 <h4>readability_run.7.dat</h4>
            <pre>
RBP(0.8)	all      	0.0194
uRBP(0.8)	all      	0.0117
uRBPgr(0.8)	all      	0.0134
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar is then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>readability_run.1.dat</h4>
<img src="./img/readability_run.1.dat.p10.png"/>
 <h4>readability_run.2.dat</h4>
<img src="./img/readability_run.2.dat.p10.png"/>
 <h4>readability_run.3.dat</h4>
<img src="./img/readability_run.3.dat.p10.png"/>
 <h4>readability_run.4.dat</h4>
<img src="./img/readability_run.4.dat.p10.png"/>
 <h4>readability_run.5.dat</h4>
<img src="./img/readability_run.5.dat.p10.png"/>
 <h4>readability_run.6.dat</h4>
<img src="./img/readability_run.6.dat.p10.png"/>
 <h4>readability_run.7.dat</h4>
<img src="./img/readability_run.7.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/ielab/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
