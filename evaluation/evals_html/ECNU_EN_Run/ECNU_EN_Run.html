<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2015 TASK 2 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2015 Task 2 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2015 TASK 2 Results - ECNU_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2015 TASK 2 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the two highest priority runs (run 2 and 3); thus all remaining runs were not sampled to form the assessment pool. This was because, unlike in previous years, submissions were greatly differing between each other, and thus lead to very large (and different) pools. As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page. Note that metrics such as MAP may not be reliable for evaluation due to the limited pool depth.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtest2015.XX id); others did not retrieve results for some queries or fail to conform to the TREC result format; finally, some retrieved the same document more than once for the same query (i.e. duplicate documents). To fairly evaluate the runs of every participant, submitted runs containing formatting errors and duplicated documents have been corrected. Duplicate documents have been removed and replaced by a <i>duplicatedid</i> string. Your runs (as evaluated here) are contained in the folder <i>runs</i> - please use these runs if you want to perform further evaluation and analysis.</br></br>Please refer to the <a href="https://sites.google.com/site/clefehealth2015/">ShARe/CLEF 2015 eHealth Evaluation Lab website</a> for additional details on the task.</p></br><p>Relevance assessments are distributed along with this webpage and are also available from the task website. Assessors judged relevance according to a three point scale: Not Relevant (label 0), Somewhat Relevant (label 1), Highly Relevant (label 2). When computing binary relevance measures (e.g. P@10 and MAP), we mapped label 0 to not relevant, and labels 1 and 2 to relevant; this  is encoded in the binary qrels named <i>qrels.clef2015.test.bin.txt</i>. Graded relevance assessments are contained in the <i>qrels.clef2015.test.graded.txt</i> file.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2015.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2015.test.bin.txt runName</pre>
 <h4>ECNU_EN_Run.1.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1612
map                   	all	0.2056
gm_map                	all	0.0657
Rprec                 	all	0.2258
bpref                 	all	0.2871
recip_rank            	all	0.5277
iprec_at_recall_0.00  	all	0.5696
iprec_at_recall_0.10  	all	0.4731
iprec_at_recall_0.20  	all	0.3620
iprec_at_recall_0.30  	all	0.3100
iprec_at_recall_0.40  	all	0.2374
iprec_at_recall_0.50  	all	0.1887
iprec_at_recall_0.60  	all	0.1286
iprec_at_recall_0.70  	all	0.0912
iprec_at_recall_0.80  	all	0.0675
iprec_at_recall_0.90  	all	0.0373
iprec_at_recall_1.00  	all	0.0175
P_5                   	all	0.3697
P_10                  	all	0.3470
P_15                  	all	0.3131
P_20                  	all	0.2955
P_30                  	all	0.2652
P_100                 	all	0.1298
P_200                 	all	0.0814
P_500                 	all	0.0430
P_1000                	all	0.0244
</pre>
 <h4>ECNU_EN_Run.10.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1621
map                   	all	0.2754
gm_map                	all	0.1098
Rprec                 	all	0.2833
bpref                 	all	0.4410
recip_rank            	all	0.7356
iprec_at_recall_0.00  	all	0.7574
iprec_at_recall_0.10  	all	0.6694
iprec_at_recall_0.20  	all	0.5316
iprec_at_recall_0.30  	all	0.3957
iprec_at_recall_0.40  	all	0.2728
iprec_at_recall_0.50  	all	0.2245
iprec_at_recall_0.60  	all	0.1595
iprec_at_recall_0.70  	all	0.1140
iprec_at_recall_0.80  	all	0.0832
iprec_at_recall_0.90  	all	0.0540
iprec_at_recall_1.00  	all	0.0420
P_5                   	all	0.5576
P_10                  	all	0.4667
P_15                  	all	0.3980
P_20                  	all	0.3477
P_30                  	all	0.2732
P_100                 	all	0.1315
P_200                 	all	0.0845
P_500                 	all	0.0425
P_1000                	all	0.0246
</pre>
 <h4>ECNU_EN_Run.2.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1622
map                   	all	0.1682
gm_map                	all	0.0471
Rprec                 	all	0.1780
bpref                 	all	0.3010
recip_rank            	all	0.4996
iprec_at_recall_0.00  	all	0.5388
iprec_at_recall_0.10  	all	0.4737
iprec_at_recall_0.20  	all	0.3130
iprec_at_recall_0.30  	all	0.2275
iprec_at_recall_0.40  	all	0.1678
iprec_at_recall_0.50  	all	0.1278
iprec_at_recall_0.60  	all	0.0914
iprec_at_recall_0.70  	all	0.0700
iprec_at_recall_0.80  	all	0.0506
iprec_at_recall_0.90  	all	0.0260
iprec_at_recall_1.00  	all	0.0101
P_5                   	all	0.3667
P_10                  	all	0.3606
P_15                  	all	0.2919
P_20                  	all	0.2492
P_30                  	all	0.1995
P_100                 	all	0.0903
P_200                 	all	0.0627
P_500                 	all	0.0415
P_1000                	all	0.0246
</pre>
 <h4>ECNU_EN_Run.3.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1665
map                   	all	0.3052
gm_map                	all	0.1308
Rprec                 	all	0.3081
bpref                 	all	0.4514
recip_rank            	all	0.7510
iprec_at_recall_0.00  	all	0.7797
iprec_at_recall_0.10  	all	0.7057
iprec_at_recall_0.20  	all	0.5882
iprec_at_recall_0.30  	all	0.4575
iprec_at_recall_0.40  	all	0.3262
iprec_at_recall_0.50  	all	0.2603
iprec_at_recall_0.60  	all	0.1741
iprec_at_recall_0.70  	all	0.1271
iprec_at_recall_0.80  	all	0.0933
iprec_at_recall_0.90  	all	0.0575
iprec_at_recall_1.00  	all	0.0436
P_5                   	all	0.5848
P_10                  	all	0.5394
P_15                  	all	0.4182
P_20                  	all	0.3621
P_30                  	all	0.2864
P_100                 	all	0.1421
P_200                 	all	0.0877
P_500                 	all	0.0440
P_1000                	all	0.0252
</pre>
 <h4>ECNU_EN_Run.4.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1627
map                   	all	0.2065
gm_map                	all	0.0553
Rprec                 	all	0.2150
bpref                 	all	0.3086
recip_rank            	all	0.4887
iprec_at_recall_0.00  	all	0.5303
iprec_at_recall_0.10  	all	0.4842
iprec_at_recall_0.20  	all	0.3677
iprec_at_recall_0.30  	all	0.2925
iprec_at_recall_0.40  	all	0.2407
iprec_at_recall_0.50  	all	0.1922
iprec_at_recall_0.60  	all	0.1370
iprec_at_recall_0.70  	all	0.1084
iprec_at_recall_0.80  	all	0.0701
iprec_at_recall_0.90  	all	0.0411
iprec_at_recall_1.00  	all	0.0189
P_5                   	all	0.3758
P_10                  	all	0.3652
P_15                  	all	0.3202
P_20                  	all	0.2864
P_30                  	all	0.2394
P_100                 	all	0.1221
P_200                 	all	0.0777
P_500                 	all	0.0425
P_1000                	all	0.0247
</pre>
 <h4>ECNU_EN_Run.5.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1452
map                   	all	0.1515
gm_map                	all	0.0404
Rprec                 	all	0.1733
bpref                 	all	0.3422
recip_rank            	all	0.5788
iprec_at_recall_0.00  	all	0.5956
iprec_at_recall_0.10  	all	0.4128
iprec_at_recall_0.20  	all	0.3005
iprec_at_recall_0.30  	all	0.2065
iprec_at_recall_0.40  	all	0.1334
iprec_at_recall_0.50  	all	0.0992
iprec_at_recall_0.60  	all	0.0702
iprec_at_recall_0.70  	all	0.0423
iprec_at_recall_0.80  	all	0.0276
iprec_at_recall_0.90  	all	0.0139
iprec_at_recall_1.00  	all	0.0038
P_5                   	all	0.3970
P_10                  	all	0.3152
P_15                  	all	0.2677
P_20                  	all	0.2318
P_30                  	all	0.1788
P_100                 	all	0.0779
P_200                 	all	0.0504
P_500                 	all	0.0353
P_1000                	all	0.0220
</pre>
 <h4>ECNU_EN_Run.6.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1621
map                   	all	0.2443
gm_map                	all	0.0959
Rprec                 	all	0.2587
bpref                 	all	0.4333
recip_rank            	all	0.6439
iprec_at_recall_0.00  	all	0.6867
iprec_at_recall_0.10  	all	0.5882
iprec_at_recall_0.20  	all	0.4738
iprec_at_recall_0.30  	all	0.3611
iprec_at_recall_0.40  	all	0.2427
iprec_at_recall_0.50  	all	0.2042
iprec_at_recall_0.60  	all	0.1533
iprec_at_recall_0.70  	all	0.0996
iprec_at_recall_0.80  	all	0.0668
iprec_at_recall_0.90  	all	0.0366
iprec_at_recall_1.00  	all	0.0234
P_5                   	all	0.4970
P_10                  	all	0.4227
P_15                  	all	0.3707
P_20                  	all	0.3205
P_30                  	all	0.2697
P_100                 	all	0.1282
P_200                 	all	0.0817
P_500                 	all	0.0424
P_1000                	all	0.0246
</pre>
 <h4>ECNU_EN_Run.7.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1499
map                   	all	0.1887
gm_map                	all	0.0602
Rprec                 	all	0.2011
bpref                 	all	0.3606
recip_rank            	all	0.5610
iprec_at_recall_0.00  	all	0.5773
iprec_at_recall_0.10  	all	0.4374
iprec_at_recall_0.20  	all	0.3430
iprec_at_recall_0.30  	all	0.2634
iprec_at_recall_0.40  	all	0.1880
iprec_at_recall_0.50  	all	0.1529
iprec_at_recall_0.60  	all	0.1105
iprec_at_recall_0.70  	all	0.0814
iprec_at_recall_0.80  	all	0.0624
iprec_at_recall_0.90  	all	0.0264
iprec_at_recall_1.00  	all	0.0090
P_5                   	all	0.4030
P_10                  	all	0.3227
P_15                  	all	0.2737
P_20                  	all	0.2402
P_30                  	all	0.2040
P_100                 	all	0.1055
P_200                 	all	0.0688
P_500                 	all	0.0378
P_1000                	all	0.0227
</pre>
 <h4>ECNU_EN_Run.8.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1553
map                   	all	0.2738
gm_map                	all	0.0902
Rprec                 	all	0.2941
bpref                 	all	0.3896
recip_rank            	all	0.6626
iprec_at_recall_0.00  	all	0.7010
iprec_at_recall_0.10  	all	0.6241
iprec_at_recall_0.20  	all	0.5206
iprec_at_recall_0.30  	all	0.4071
iprec_at_recall_0.40  	all	0.3016
iprec_at_recall_0.50  	all	0.2478
iprec_at_recall_0.60  	all	0.1752
iprec_at_recall_0.70  	all	0.1183
iprec_at_recall_0.80  	all	0.0782
iprec_at_recall_0.90  	all	0.0432
iprec_at_recall_1.00  	all	0.0177
P_5                   	all	0.5152
P_10                  	all	0.4530
P_15                  	all	0.3939
P_20                  	all	0.3470
P_30                  	all	0.2955
P_100                 	all	0.1506
P_200                 	all	0.0914
P_500                 	all	0.0434
P_1000                	all	0.0235
</pre>
 <h4>ECNU_EN_Run.9.dat</h4>
            <pre>
runid                 	all	ecnuEn
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1629
map                   	all	0.2280
gm_map                	all	0.0624
Rprec                 	all	0.2375
bpref                 	all	0.3080
recip_rank            	all	0.5103
iprec_at_recall_0.00  	all	0.5442
iprec_at_recall_0.10  	all	0.4777
iprec_at_recall_0.20  	all	0.4009
iprec_at_recall_0.30  	all	0.3360
iprec_at_recall_0.40  	all	0.2740
iprec_at_recall_0.50  	all	0.2096
iprec_at_recall_0.60  	all	0.1553
iprec_at_recall_0.70  	all	0.1238
iprec_at_recall_0.80  	all	0.0877
iprec_at_recall_0.90  	all	0.0520
iprec_at_recall_1.00  	all	0.0190
P_5                   	all	0.3909
P_10                  	all	0.3606
P_15                  	all	0.3525
P_20                  	all	0.3121
P_30                  	all	0.2616
P_100                 	all	0.1395
P_200                 	all	0.0852
P_500                 	all	0.0432
P_1000                	all	0.0247
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2015.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2015.test.graded.txt runName</pre></br><p>This computes nDCG according to Jarvelin and Kekalainen (ACM ToIS v. 20, pp. 422-446, 2002). Gain values are the relevance values in the qrels file (i.e. label 0 corresponds to gain 0, label 1 to gain 1 and label 2 to gain 2).</p>
 <h4>ECNU_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3104
ndcg_cut_10           	all	0.3144
ndcg_cut_15           	all	0.3041
ndcg_cut_20           	all	0.3054
ndcg_cut_30           	all	0.3072
ndcg_cut_100          	all	0.3285
ndcg_cut_200          	all	0.3685
ndcg_cut_500          	all	0.4184
ndcg_cut_1000         	all	0.4459
</pre>
 <h4>ECNU_EN_Run.10.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4773
ndcg_cut_10           	all	0.4525
ndcg_cut_15           	all	0.4325
ndcg_cut_20           	all	0.4140
ndcg_cut_30           	all	0.3953
ndcg_cut_100          	all	0.4193
ndcg_cut_200          	all	0.4634
ndcg_cut_500          	all	0.5047
ndcg_cut_1000         	all	0.5305
</pre>
 <h4>ECNU_EN_Run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3037
ndcg_cut_10           	all	0.3220
ndcg_cut_15           	all	0.2970
ndcg_cut_20           	all	0.2800
ndcg_cut_30           	all	0.2683
ndcg_cut_100          	all	0.2713
ndcg_cut_200          	all	0.3074
ndcg_cut_500          	all	0.3851
ndcg_cut_1000         	all	0.4224
</pre>
 <h4>ECNU_EN_Run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.5023
ndcg_cut_10           	all	0.5086
ndcg_cut_15           	all	0.4672
ndcg_cut_20           	all	0.4482
ndcg_cut_30           	all	0.4269
ndcg_cut_100          	all	0.4545
ndcg_cut_200          	all	0.4942
ndcg_cut_500          	all	0.5375
ndcg_cut_1000         	all	0.5646
</pre>
 <h4>ECNU_EN_Run.4.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3003
ndcg_cut_10           	all	0.3168
ndcg_cut_15           	all	0.3059
ndcg_cut_20           	all	0.3018
ndcg_cut_30           	all	0.2956
ndcg_cut_100          	all	0.3189
ndcg_cut_200          	all	0.3525
ndcg_cut_500          	all	0.4074
ndcg_cut_1000         	all	0.4407
</pre>
 <h4>ECNU_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3343
ndcg_cut_10           	all	0.3006
ndcg_cut_15           	all	0.2859
ndcg_cut_20           	all	0.2698
ndcg_cut_30           	all	0.2531
ndcg_cut_100          	all	0.2529
ndcg_cut_200          	all	0.2845
ndcg_cut_500          	all	0.3580
ndcg_cut_1000         	all	0.3972
</pre>
 <h4>ECNU_EN_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4093
ndcg_cut_10           	all	0.3978
ndcg_cut_15           	all	0.3883
ndcg_cut_20           	all	0.3689
ndcg_cut_30           	all	0.3617
ndcg_cut_100          	all	0.3819
ndcg_cut_200          	all	0.4282
ndcg_cut_500          	all	0.4746
ndcg_cut_1000         	all	0.5000
</pre>
 <h4>ECNU_EN_Run.7.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3254
ndcg_cut_10           	all	0.3004
ndcg_cut_15           	all	0.2851
ndcg_cut_20           	all	0.2734
ndcg_cut_30           	all	0.2768
ndcg_cut_100          	all	0.3138
ndcg_cut_200          	all	0.3495
ndcg_cut_500          	all	0.3979
ndcg_cut_1000         	all	0.4303
</pre>
 <h4>ECNU_EN_Run.8.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4360
ndcg_cut_10           	all	0.4226
ndcg_cut_15           	all	0.4067
ndcg_cut_20           	all	0.3923
ndcg_cut_30           	all	0.3897
ndcg_cut_100          	all	0.4261
ndcg_cut_200          	all	0.4609
ndcg_cut_500          	all	0.4901
ndcg_cut_1000         	all	0.5065
</pre>
 <h4>ECNU_EN_Run.9.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3140
ndcg_cut_10           	all	0.3203
ndcg_cut_15           	all	0.3279
ndcg_cut_20           	all	0.3149
ndcg_cut_30           	all	0.3067
ndcg_cut_100          	all	0.3461
ndcg_cut_200          	all	0.3816
ndcg_cut_500          	all	0.4243
ndcg_cut_1000         	all	0.4541
</pre>

<h2>Readability-biased evaluation</h2>
        <p>For this year task, we have considered the factor of understandability of information (or readability) within the evaluation of the submissions, along with the topicality factor (normally referred to as (topical) relevance). Thus, along with (topical) relevance assessments (qrels), we have collected readability assessments (qread). These assessments were provided by judges along with the relevance assessments; however readability was assumed to be assessed independently of (topical) relevance. To account for understandability/readability in the evaluation, we have computed an understandability biased measure, uRBP, as defined in Zuccon&Koopman, <a href="http://zuccon.net/publications/medIR2014_healthsearch_readability.pdf">"Integrating understandability in the evaluation of consumer health search engines"</a>, MedIR 2014. We refer to that publication for the motivations and the details of the measure; note however that we did not use automated readability measures to estimate readability - we instead had actual readability assessments from the relevance assessors. Readability assessments were given on a 4 point scale (from 0 to 3): It is very technical and difficult to read and understand (label 0), It is somewhat technical and difficult to read and understand (label 1), It is somewhat easy to read and understand (label 2), It is very easy to read and understand (label 3).</p><p>The results below have been obtained with the binary relevance assessments (i.e. qrels.clef2015.test.bin.txt) and the graded readability assessments (i.e. i.e. qread.clef2015.test.graded.txt), and ubire-v0.1.0 as distributed on <a href="https://github.com/ielab/ubire">GitHub</a>.</br>The tool was ran as follows:</p>
<pre>java -jar /tools/ubire.0.1.jar --qrels-file=qrels/qrels.clef2015.test.bin.txt --qread-file=qrels/qread.clef2015.test.graded.txt --readability --rbp-p=0.8 --ranking-file=runName</pre></br><p>This computes RBP, and two versions of uRBP. The user persistence parameter <i>p</i> of RBP (and uRBP) was set to 0.8 following Park&Zhang, <a href="http://goanna.cs.rmit.edu.au/~aht/adcs2007/papers/17N.PDF">"On the distribution of user persistence for rank-biased precision"</a>, ADCS 2007.  uRBP has been computed by using user model 1 of Zuccon&Koopman with threshold=2, i.e. documents with a readability score of 0 or 1 where deemed not readable and thus had P(U|k)=0, while documents with a readability score of 2 or 3 where deemed readable and thus had P(U|k)=1. uRBPgr has been computed by mapping graded readability scores to different probability values, in particular: readability of 0 was assigned P(U|k)=0, readability of 1 was assigned P(U|k)=0.4, readability of 2 was assigned P(U|k)=0.8, readability of 3 was assigned P(U|k)=1.</br>Note that we are still experimenting with these readability-biased measures and thus observations made with the provided measures may not be conclusive.</p>
 <h4>ECNU_EN_Run.1.dat</h4>
            <pre>
RBP(0.8)	all      	0.3549
uRBP(0.8)	all      	0.3080
uRBPgr(0.8)	all      	0.2971
</pre>
 <h4>ECNU_EN_Run.10.dat</h4>
            <pre>
RBP(0.8)	all      	0.4955
uRBP(0.8)	all      	0.3768
uRBPgr(0.8)	all      	0.3873
</pre>
 <h4>ECNU_EN_Run.2.dat</h4>
            <pre>
RBP(0.8)	all      	0.3527
uRBP(0.8)	all      	0.2917
uRBPgr(0.8)	all      	0.2830
</pre>
 <h4>ECNU_EN_Run.3.dat</h4>
            <pre>
RBP(0.8)	all      	0.5339
uRBP(0.8)	all      	0.3877
uRBPgr(0.8)	all      	0.4046
</pre>
 <h4>ECNU_EN_Run.4.dat</h4>
            <pre>
RBP(0.8)	all      	0.3638
uRBP(0.8)	all      	0.3103
uRBPgr(0.8)	all      	0.2990
</pre>
 <h4>ECNU_EN_Run.5.dat</h4>
            <pre>
RBP(0.8)	all      	0.3531
uRBP(0.8)	all      	0.2771
uRBPgr(0.8)	all      	0.2804
</pre>
 <h4>ECNU_EN_Run.6.dat</h4>
            <pre>
RBP(0.8)	all      	0.4459
uRBP(0.8)	all      	0.3374
uRBPgr(0.8)	all      	0.3453
</pre>
 <h4>ECNU_EN_Run.7.dat</h4>
            <pre>
RBP(0.8)	all      	0.3548
uRBP(0.8)	all      	0.2841
uRBPgr(0.8)	all      	0.2869
</pre>
 <h4>ECNU_EN_Run.8.dat</h4>
            <pre>
RBP(0.8)	all      	0.4472
uRBP(0.8)	all      	0.3273
uRBPgr(0.8)	all      	0.3373
</pre>
 <h4>ECNU_EN_Run.9.dat</h4>
            <pre>
RBP(0.8)	all      	0.3730
uRBP(0.8)	all      	0.3249
uRBPgr(0.8)	all      	0.3107
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar is then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>ECNU_EN_Run.1.dat</h4>
<img src="./img/ECNU_EN_Run.1.dat.p10.png"/>
 <h4>ECNU_EN_Run.10.dat</h4>
<img src="./img/ECNU_EN_Run.10.dat.p10.png"/>
 <h4>ECNU_EN_Run.2.dat</h4>
<img src="./img/ECNU_EN_Run.2.dat.p10.png"/>
 <h4>ECNU_EN_Run.3.dat</h4>
<img src="./img/ECNU_EN_Run.3.dat.p10.png"/>
 <h4>ECNU_EN_Run.4.dat</h4>
<img src="./img/ECNU_EN_Run.4.dat.p10.png"/>
 <h4>ECNU_EN_Run.5.dat</h4>
<img src="./img/ECNU_EN_Run.5.dat.p10.png"/>
 <h4>ECNU_EN_Run.6.dat</h4>
<img src="./img/ECNU_EN_Run.6.dat.p10.png"/>
 <h4>ECNU_EN_Run.7.dat</h4>
<img src="./img/ECNU_EN_Run.7.dat.p10.png"/>
 <h4>ECNU_EN_Run.8.dat</h4>
<img src="./img/ECNU_EN_Run.8.dat.p10.png"/>
 <h4>ECNU_EN_Run.9.dat</h4>
<img src="./img/ECNU_EN_Run.9.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/ielab/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
