<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2015 TASK 2 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2015 Task 2 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2015 TASK 2 Results - GRIUM_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2015 TASK 2 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the two highest priority runs (run 2 and 3); thus all remaining runs were not sampled to form the assessment pool. This was because, unlike in previous years, submissions were greatly differing between each other, and thus lead to very large (and different) pools. As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page. Note that metrics such as MAP may not be reliable for evaluation due to the limited pool depth.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtest2015.XX id); others did not retrieve results for some queries or fail to conform to the TREC result format; finally, some retrieved the same document more than once for the same query (i.e. duplicate documents). To fairly evaluate the runs of every participant, submitted runs containing formatting errors and duplicated documents have been corrected. Duplicate documents have been removed and replaced by a <i>duplicatedid</i> string. Your runs (as evaluated here) are contained in the folder <i>runs</i> - please use these runs if you want to perform further evaluation and analysis.</br></br>Please refer to the <a href="https://sites.google.com/site/clefehealth2015/">ShARe/CLEF 2015 eHealth Evaluation Lab website</a> for additional details on the task.</p></br><p>Relevance assessments are distributed along with this webpage and are also available from the task website. Assessors judged relevance according to a three point scale: Not Relevant (label 0), Somewhat Relevant (label 1), Highly Relevant (label 2). When computing binary relevance measures (e.g. P@10 and MAP), we mapped label 0 to not relevant, and labels 1 and 2 to relevant; this  is encoded in the binary qrels named <i>qrels.clef2015.test.bin.txt</i>. Graded relevance assessments are contained in the <i>qrels.clef2015.test.graded.txt</i> file.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2015.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2015.test.bin.txt runName</pre>
 <h4>GRIUM_EN_Run.1.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_baseline
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1477
map                   	all	0.1971
gm_map                	all	0.0776
Rprec                 	all	0.2126
bpref                 	all	0.2702
recip_rank            	all	0.5193
iprec_at_recall_0.00  	all	0.5632
iprec_at_recall_0.10  	all	0.4657
iprec_at_recall_0.20  	all	0.3451
iprec_at_recall_0.30  	all	0.2734
iprec_at_recall_0.40  	all	0.2121
iprec_at_recall_0.50  	all	0.1677
iprec_at_recall_0.60  	all	0.1363
iprec_at_recall_0.70  	all	0.1042
iprec_at_recall_0.80  	all	0.0708
iprec_at_recall_0.90  	all	0.0366
iprec_at_recall_1.00  	all	0.0207
P_5                   	all	0.3273
P_10                  	all	0.3136
P_15                  	all	0.2869
P_20                  	all	0.2689
P_30                  	all	0.2359
P_100                 	all	0.1236
P_200                 	all	0.0800
P_500                 	all	0.0396
P_1000                	all	0.0224
</pre>
 <h4>GRIUM_EN_Run.2.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_combineUMLSwikiPara1
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1472
map                   	all	0.1982
gm_map                	all	0.0717
Rprec                 	all	0.2107
bpref                 	all	0.2717
recip_rank            	all	0.5256
iprec_at_recall_0.00  	all	0.5700
iprec_at_recall_0.10  	all	0.4677
iprec_at_recall_0.20  	all	0.3423
iprec_at_recall_0.30  	all	0.2731
iprec_at_recall_0.40  	all	0.2103
iprec_at_recall_0.50  	all	0.1682
iprec_at_recall_0.60  	all	0.1356
iprec_at_recall_0.70  	all	0.1042
iprec_at_recall_0.80  	all	0.0740
iprec_at_recall_0.90  	all	0.0354
iprec_at_recall_1.00  	all	0.0202
P_5                   	all	0.3424
P_10                  	all	0.3091
P_15                  	all	0.2909
P_20                  	all	0.2720
P_30                  	all	0.2343
P_100                 	all	0.1244
P_200                 	all	0.0804
P_500                 	all	0.0399
P_1000                	all	0.0223
</pre>
 <h4>GRIUM_EN_Run.3.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_UMLSDEFPara1
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1471
map                   	all	0.1982
gm_map                	all	0.0724
Rprec                 	all	0.2119
bpref                 	all	0.2712
recip_rank            	all	0.5269
iprec_at_recall_0.00  	all	0.5710
iprec_at_recall_0.10  	all	0.4716
iprec_at_recall_0.20  	all	0.3450
iprec_at_recall_0.30  	all	0.2732
iprec_at_recall_0.40  	all	0.2107
iprec_at_recall_0.50  	all	0.1687
iprec_at_recall_0.60  	all	0.1364
iprec_at_recall_0.70  	all	0.1032
iprec_at_recall_0.80  	all	0.0708
iprec_at_recall_0.90  	all	0.0364
iprec_at_recall_1.00  	all	0.0207
P_5                   	all	0.3333
P_10                  	all	0.3167
P_15                  	all	0.2919
P_20                  	all	0.2689
P_30                  	all	0.2369
P_100                 	all	0.1247
P_200                 	all	0.0800
P_500                 	all	0.0396
P_1000                	all	0.0223
</pre>
 <h4>GRIUM_EN_Run.4.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_wikiabsPara1
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1494
map                   	all	0.1920
gm_map                	all	0.0691
Rprec                 	all	0.2082
bpref                 	all	0.2698
recip_rank            	all	0.5222
iprec_at_recall_0.00  	all	0.5665
iprec_at_recall_0.10  	all	0.4545
iprec_at_recall_0.20  	all	0.3289
iprec_at_recall_0.30  	all	0.2544
iprec_at_recall_0.40  	all	0.1961
iprec_at_recall_0.50  	all	0.1591
iprec_at_recall_0.60  	all	0.1289
iprec_at_recall_0.70  	all	0.1022
iprec_at_recall_0.80  	all	0.0772
iprec_at_recall_0.90  	all	0.0348
iprec_at_recall_1.00  	all	0.0194
P_5                   	all	0.3394
P_10                  	all	0.3030
P_15                  	all	0.2808
P_20                  	all	0.2667
P_30                  	all	0.2222
P_100                 	all	0.1226
P_200                 	all	0.0788
P_500                 	all	0.0401
P_1000                	all	0.0226
</pre>
 <h4>GRIUM_EN_Run.5.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_combineUMLSwikiPara2
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1472
map                   	all	0.1968
gm_map                	all	0.0709
Rprec                 	all	0.2081
bpref                 	all	0.2721
recip_rank            	all	0.5139
iprec_at_recall_0.00  	all	0.5615
iprec_at_recall_0.10  	all	0.4686
iprec_at_recall_0.20  	all	0.3405
iprec_at_recall_0.30  	all	0.2717
iprec_at_recall_0.40  	all	0.2094
iprec_at_recall_0.50  	all	0.1684
iprec_at_recall_0.60  	all	0.1344
iprec_at_recall_0.70  	all	0.1034
iprec_at_recall_0.80  	all	0.0749
iprec_at_recall_0.90  	all	0.0342
iprec_at_recall_1.00  	all	0.0202
P_5                   	all	0.3424
P_10                  	all	0.3045
P_15                  	all	0.2869
P_20                  	all	0.2674
P_30                  	all	0.2308
P_100                 	all	0.1241
P_200                 	all	0.0803
P_500                 	all	0.0403
P_1000                	all	0.0223
</pre>
 <h4>GRIUM_EN_Run.6.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_UMLSDEFPara2
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1473
map                   	all	0.1982
gm_map                	all	0.0722
Rprec                 	all	0.2095
bpref                 	all	0.2715
recip_rank            	all	0.5267
iprec_at_recall_0.00  	all	0.5707
iprec_at_recall_0.10  	all	0.4714
iprec_at_recall_0.20  	all	0.3410
iprec_at_recall_0.30  	all	0.2723
iprec_at_recall_0.40  	all	0.2100
iprec_at_recall_0.50  	all	0.1688
iprec_at_recall_0.60  	all	0.1363
iprec_at_recall_0.70  	all	0.1023
iprec_at_recall_0.80  	all	0.0713
iprec_at_recall_0.90  	all	0.0364
iprec_at_recall_1.00  	all	0.0207
P_5                   	all	0.3424
P_10                  	all	0.3182
P_15                  	all	0.2899
P_20                  	all	0.2682
P_30                  	all	0.2354
P_100                 	all	0.1247
P_200                 	all	0.0805
P_500                 	all	0.0402
P_1000                	all	0.0223
</pre>
 <h4>GRIUM_EN_Run.7.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_wikiabsPara2
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1474
map                   	all	0.1971
gm_map                	all	0.0713
Rprec                 	all	0.2106
bpref                 	all	0.2705
recip_rank            	all	0.5149
iprec_at_recall_0.00  	all	0.5637
iprec_at_recall_0.10  	all	0.4689
iprec_at_recall_0.20  	all	0.3457
iprec_at_recall_0.30  	all	0.2727
iprec_at_recall_0.40  	all	0.2096
iprec_at_recall_0.50  	all	0.1658
iprec_at_recall_0.60  	all	0.1339
iprec_at_recall_0.70  	all	0.1032
iprec_at_recall_0.80  	all	0.0756
iprec_at_recall_0.90  	all	0.0341
iprec_at_recall_1.00  	all	0.0203
P_5                   	all	0.3394
P_10                  	all	0.3061
P_15                  	all	0.2929
P_20                  	all	0.2727
P_30                  	all	0.2343
P_100                 	all	0.1233
P_200                 	all	0.0798
P_500                 	all	0.0398
P_1000                	all	0.0223
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2015.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2015.test.graded.txt runName</pre></br><p>This computes nDCG according to Jarvelin and Kekalainen (ACM ToIS v. 20, pp. 422-446, 2002). Gain values are the relevance values in the qrels file (i.e. label 0 corresponds to gain 0, label 1 to gain 1 and label 2 to gain 2).</p>
 <h4>GRIUM_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2717
ndcg_cut_10           	all	0.2875
ndcg_cut_15           	all	0.2870
ndcg_cut_20           	all	0.2894
ndcg_cut_30           	all	0.2931
ndcg_cut_100          	all	0.3252
ndcg_cut_200          	all	0.3709
ndcg_cut_500          	all	0.4059
ndcg_cut_1000         	all	0.4307
</pre>
 <h4>GRIUM_EN_Run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2870
ndcg_cut_10           	all	0.2850
ndcg_cut_15           	all	0.2924
ndcg_cut_20           	all	0.2916
ndcg_cut_30           	all	0.2934
ndcg_cut_100          	all	0.3262
ndcg_cut_200          	all	0.3729
ndcg_cut_500          	all	0.4085
ndcg_cut_1000         	all	0.4303
</pre>
 <h4>GRIUM_EN_Run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2787
ndcg_cut_10           	all	0.2913
ndcg_cut_15           	all	0.2917
ndcg_cut_20           	all	0.2897
ndcg_cut_30           	all	0.2947
ndcg_cut_100          	all	0.3279
ndcg_cut_200          	all	0.3724
ndcg_cut_500          	all	0.4076
ndcg_cut_1000         	all	0.4303
</pre>
 <h4>GRIUM_EN_Run.4.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2853
ndcg_cut_10           	all	0.2788
ndcg_cut_15           	all	0.2791
ndcg_cut_20           	all	0.2841
ndcg_cut_30           	all	0.2795
ndcg_cut_100          	all	0.3202
ndcg_cut_200          	all	0.3630
ndcg_cut_500          	all	0.4029
ndcg_cut_1000         	all	0.4259
</pre>
 <h4>GRIUM_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2841
ndcg_cut_10           	all	0.2803
ndcg_cut_15           	all	0.2876
ndcg_cut_20           	all	0.2868
ndcg_cut_30           	all	0.2867
ndcg_cut_100          	all	0.3237
ndcg_cut_200          	all	0.3699
ndcg_cut_500          	all	0.4069
ndcg_cut_1000         	all	0.4273
</pre>
 <h4>GRIUM_EN_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2868
ndcg_cut_10           	all	0.2944
ndcg_cut_15           	all	0.2913
ndcg_cut_20           	all	0.2902
ndcg_cut_30           	all	0.2936
ndcg_cut_100          	all	0.3273
ndcg_cut_200          	all	0.3732
ndcg_cut_500          	all	0.4099
ndcg_cut_1000         	all	0.4312
</pre>
 <h4>GRIUM_EN_Run.7.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2803
ndcg_cut_10           	all	0.2798
ndcg_cut_15           	all	0.2900
ndcg_cut_20           	all	0.2904
ndcg_cut_30           	all	0.2915
ndcg_cut_100          	all	0.3226
ndcg_cut_200          	all	0.3702
ndcg_cut_500          	all	0.4054
ndcg_cut_1000         	all	0.4275
</pre>

<h2>Readability-biased evaluation</h2>
        <p>For this year task, we have considered the factor of understandability of information (or readability) within the evaluation of the submissions, along with the topicality factor (normally referred to as (topical) relevance). Thus, along with (topical) relevance assessments (qrels), we have collected readability assessments (qread). These assessments were provided by judges along with the relevance assessments; however readability was assumed to be assessed independently of (topical) relevance. To account for understandability/readability in the evaluation, we have computed an understandability biased measure, uRBP, as defined in Zuccon&Koopman, <a href="http://zuccon.net/publications/medIR2014_healthsearch_readability.pdf">"Integrating understandability in the evaluation of consumer health search engines"</a>, MedIR 2014. We refer to that publication for the motivations and the details of the measure; note however that we did not use automated readability measures to estimate readability - we instead had actual readability assessments from the relevance assessors. Readability assessments were given on a 4 point scale (from 0 to 3): It is very technical and difficult to read and understand (label 0), It is somewhat technical and difficult to read and understand (label 1), It is somewhat easy to read and understand (label 2), It is very easy to read and understand (label 3).</p><p>The results below have been obtained with the binary relevance assessments (i.e. qrels.clef2015.test.bin.txt) and the graded readability assessments (i.e. i.e. qread.clef2015.test.graded.txt), and ubire-v0.1.0 as distributed on <a href="https://github.com/ielab/ubire">GitHub</a>.</br>The tool was ran as follows:</p>
<pre>java -jar /tools/ubire.0.1.jar --qrels-file=qrels/qrels.clef2015.test.bin.txt --qread-file=qrels/qread.clef2015.test.graded.txt --readability --rbp-p=0.8 --ranking-file=runName</pre></br><p>This computes RBP, and two versions of uRBP. The user persistence parameter <i>p</i> of RBP (and uRBP) was set to 0.8 following Park&Zhang, <a href="http://goanna.cs.rmit.edu.au/~aht/adcs2007/papers/17N.PDF">"On the distribution of user persistence for rank-biased precision"</a>, ADCS 2007.  uRBP has been computed by using user model 1 of Zuccon&Koopman with threshold=2, i.e. documents with a readability score of 0 or 1 where deemed not readable and thus had P(U|k)=0, while documents with a readability score of 2 or 3 where deemed readable and thus had P(U|k)=1. uRBPgr has been computed by mapping graded readability scores to different probability values, in particular: readability of 0 was assigned P(U|k)=0, readability of 1 was assigned P(U|k)=0.4, readability of 2 was assigned P(U|k)=0.8, readability of 3 was assigned P(U|k)=1.</br>Note that we are still experimenting with these readability-biased measures and thus observations made with the provided measures may not be conclusive.</p>
 <h4>GRIUM_EN_Run.1.dat</h4>
            <pre>
RBP(0.8)	all      	0.3249
uRBP(0.8)	all      	0.2725
uRBPgr(0.8)	all      	0.2700
</pre>
 <h4>GRIUM_EN_Run.2.dat</h4>
            <pre>
RBP(0.8)	all      	0.3305
uRBP(0.8)	all      	0.2809
uRBPgr(0.8)	all      	0.2768
</pre>
 <h4>GRIUM_EN_Run.3.dat</h4>
            <pre>
RBP(0.8)	all      	0.3296
uRBP(0.8)	all      	0.2775
uRBPgr(0.8)	all      	0.2745
</pre>
 <h4>GRIUM_EN_Run.4.dat</h4>
            <pre>
RBP(0.8)	all      	0.3244
uRBP(0.8)	all      	0.2778
uRBPgr(0.8)	all      	0.2719
</pre>
 <h4>GRIUM_EN_Run.5.dat</h4>
            <pre>
RBP(0.8)	all      	0.3278
uRBP(0.8)	all      	0.2780
uRBPgr(0.8)	all      	0.2744
</pre>
 <h4>GRIUM_EN_Run.6.dat</h4>
            <pre>
RBP(0.8)	all      	0.3306
uRBP(0.8)	all      	0.2791
uRBPgr(0.8)	all      	0.2761
</pre>
 <h4>GRIUM_EN_Run.7.dat</h4>
            <pre>
RBP(0.8)	all      	0.3272
uRBP(0.8)	all      	0.2774
uRBPgr(0.8)	all      	0.2739
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar is then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>GRIUM_EN_Run.1.dat</h4>
<img src="./img/GRIUM_EN_Run.1.dat.p10.png"/>
 <h4>GRIUM_EN_Run.2.dat</h4>
<img src="./img/GRIUM_EN_Run.2.dat.p10.png"/>
 <h4>GRIUM_EN_Run.3.dat</h4>
<img src="./img/GRIUM_EN_Run.3.dat.p10.png"/>
 <h4>GRIUM_EN_Run.4.dat</h4>
<img src="./img/GRIUM_EN_Run.4.dat.p10.png"/>
 <h4>GRIUM_EN_Run.5.dat</h4>
<img src="./img/GRIUM_EN_Run.5.dat.p10.png"/>
 <h4>GRIUM_EN_Run.6.dat</h4>
<img src="./img/GRIUM_EN_Run.6.dat.p10.png"/>
 <h4>GRIUM_EN_Run.7.dat</h4>
<img src="./img/GRIUM_EN_Run.7.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/ielab/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
