<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2015 TASK 2 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2015 Task 2 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2015 TASK 2 Results - Miracl_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2015 TASK 2 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the two highest priority runs (run 2 and 3); thus all remaining runs were not sampled to form the assessment pool. This was because, unlike in previous years, submissions were greatly differing between each other, and thus lead to very large (and different) pools. As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page. Note that metrics such as MAP may not be reliable for evaluation due to the limited pool depth.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtest2015.XX id); others did not retrieve results for some queries or fail to conform to the TREC result format; finally, some retrieved the same document more than once for the same query (i.e. duplicate documents). To fairly evaluate the runs of every participant, submitted runs containing formatting errors and duplicated documents have been corrected. Duplicate documents have been removed and replaced by a <i>duplicatedid</i> string. Your runs (as evaluated here) are contained in the folder <i>runs</i> - please use these runs if you want to perform further evaluation and analysis.</br></br>Please refer to the <a href="https://sites.google.com/site/clefehealth2015/">ShARe/CLEF 2015 eHealth Evaluation Lab website</a> for additional details on the task.</p></br><p>Relevance assessments are distributed along with this webpage and are also available from the task website. Assessors judged relevance according to a three point scale: Not Relevant (label 0), Somewhat Relevant (label 1), Highly Relevant (label 2). When computing binary relevance measures (e.g. P@10 and MAP), we mapped label 0 to not relevant, and labels 1 and 2 to relevant; this  is encoded in the binary qrels named <i>qrels.clef2015.test.bin.txt</i>. Graded relevance assessments are contained in the <i>qrels.clef2015.test.graded.txt</i> file.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2015.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2015.test.bin.txt runName</pre>
 <h4>Miracl_EN_Run.1.dat</h4>
            <pre>
runid                 	all	VSM
num_q                 	all	66
num_ret               	all	65936
num_rel               	all	1972
num_rel_ret           	all	1459
map                   	all	0.1697
gm_map                	all	0.0530
Rprec                 	all	0.2024
bpref                 	all	0.2433
recip_rank            	all	0.4740
iprec_at_recall_0.00  	all	0.5213
iprec_at_recall_0.10  	all	0.4198
iprec_at_recall_0.20  	all	0.2947
iprec_at_recall_0.30  	all	0.2422
iprec_at_recall_0.40  	all	0.1889
iprec_at_recall_0.50  	all	0.1418
iprec_at_recall_0.60  	all	0.1041
iprec_at_recall_0.70  	all	0.0703
iprec_at_recall_0.80  	all	0.0462
iprec_at_recall_0.90  	all	0.0172
iprec_at_recall_1.00  	all	0.0079
P_5                   	all	0.3606
P_10                  	all	0.3212
P_15                  	all	0.2848
P_20                  	all	0.2523
P_30                  	all	0.2207
P_100                 	all	0.1224
P_200                 	all	0.0748
P_500                 	all	0.0382
P_1000                	all	0.0221
</pre>
 <h4>Miracl_EN_Run.2.dat</h4>
            <pre>
runid                 	all	VSMLS
num_q                 	all	66
num_ret               	all	65831
num_rel               	all	1972
num_rel_ret           	all	977
map                   	all	0.0804
gm_map                	all	0.0094
Rprec                 	all	0.1033
bpref                 	all	0.1731
recip_rank            	all	0.3141
iprec_at_recall_0.00  	all	0.3579
iprec_at_recall_0.10  	all	0.2475
iprec_at_recall_0.20  	all	0.1254
iprec_at_recall_0.30  	all	0.1006
iprec_at_recall_0.40  	all	0.0742
iprec_at_recall_0.50  	all	0.0501
iprec_at_recall_0.60  	all	0.0307
iprec_at_recall_0.70  	all	0.0182
iprec_at_recall_0.80  	all	0.0108
iprec_at_recall_0.90  	all	0.0060
iprec_at_recall_1.00  	all	0.0059
P_5                   	all	0.2606
P_10                  	all	0.2424
P_15                  	all	0.1879
P_20                  	all	0.1591
P_30                  	all	0.1227
P_100                 	all	0.0673
P_200                 	all	0.0429
P_500                 	all	0.0232
P_1000                	all	0.0148
</pre>
 <h4>Miracl_EN_Run.3.dat</h4>
            <pre>
runid                 	all	VSMLS100
num_q                 	all	66
num_ret               	all	65831
num_rel               	all	1972
num_rel_ret           	all	1050
map                   	all	0.0844
gm_map                	all	0.0128
Rprec                 	all	0.1175
bpref                 	all	0.1866
recip_rank            	all	0.3141
iprec_at_recall_0.00  	all	0.3674
iprec_at_recall_0.10  	all	0.2629
iprec_at_recall_0.20  	all	0.1541
iprec_at_recall_0.30  	all	0.1143
iprec_at_recall_0.40  	all	0.0783
iprec_at_recall_0.50  	all	0.0567
iprec_at_recall_0.60  	all	0.0336
iprec_at_recall_0.70  	all	0.0156
iprec_at_recall_0.80  	all	0.0089
iprec_at_recall_0.90  	all	0.0037
iprec_at_recall_1.00  	all	0.0023
P_5                   	all	0.2303
P_10                  	all	0.2515
P_15                  	all	0.1939
P_20                  	all	0.1720
P_30                  	all	0.1404
P_100                 	all	0.0814
P_200                 	all	0.0495
P_500                 	all	0.0254
P_1000                	all	0.0159
</pre>
 <h4>Miracl_EN_Run.4.dat</h4>
            <pre>
runid                 	all	VSMLS100-005
num_q                 	all	66
num_ret               	all	65831
num_rel               	all	1972
num_rel_ret           	all	1201
map                   	all	0.0975
gm_map                	all	0.0192
Rprec                 	all	0.1227
bpref                 	all	0.2202
recip_rank            	all	0.3328
iprec_at_recall_0.00  	all	0.3855
iprec_at_recall_0.10  	all	0.2635
iprec_at_recall_0.20  	all	0.1784
iprec_at_recall_0.30  	all	0.1322
iprec_at_recall_0.40  	all	0.1041
iprec_at_recall_0.50  	all	0.0734
iprec_at_recall_0.60  	all	0.0470
iprec_at_recall_0.70  	all	0.0315
iprec_at_recall_0.80  	all	0.0192
iprec_at_recall_0.90  	all	0.0078
iprec_at_recall_1.00  	all	0.0039
P_5                   	all	0.2242
P_10                  	all	0.1894
P_15                  	all	0.1768
P_20                  	all	0.1583
P_30                  	all	0.1379
P_100                 	all	0.0826
P_200                 	all	0.0542
P_500                 	all	0.0295
P_1000                	all	0.0182
</pre>
 <h4>Miracl_EN_Run.5.dat</h4>
            <pre>
runid                 	all	VSMLS100-03
num_q                 	all	66
num_ret               	all	65831
num_rel               	all	1972
num_rel_ret           	all	1448
map                   	all	0.1555
gm_map                	all	0.0484
Rprec                 	all	0.1790
bpref                 	all	0.2496
recip_rank            	all	0.4462
iprec_at_recall_0.00  	all	0.4907
iprec_at_recall_0.10  	all	0.3771
iprec_at_recall_0.20  	all	0.2770
iprec_at_recall_0.30  	all	0.2236
iprec_at_recall_0.40  	all	0.1769
iprec_at_recall_0.50  	all	0.1275
iprec_at_recall_0.60  	all	0.0975
iprec_at_recall_0.70  	all	0.0663
iprec_at_recall_0.80  	all	0.0422
iprec_at_recall_0.90  	all	0.0165
iprec_at_recall_1.00  	all	0.0090
P_5                   	all	0.3121
P_10                  	all	0.2939
P_15                  	all	0.2727
P_20                  	all	0.2455
P_30                  	all	0.2121
P_100                 	all	0.1173
P_200                 	all	0.0725
P_500                 	all	0.0380
P_1000                	all	0.0219
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2015.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2015.test.graded.txt runName</pre></br><p>This computes nDCG according to Jarvelin and Kekalainen (ACM ToIS v. 20, pp. 422-446, 2002). Gain values are the relevance values in the qrels file (i.e. label 0 corresponds to gain 0, label 1 to gain 1 and label 2 to gain 2).</p>
 <h4>Miracl_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2819
ndcg_cut_10           	all	0.2787
ndcg_cut_15           	all	0.2688
ndcg_cut_20           	all	0.2580
ndcg_cut_30           	all	0.2545
ndcg_cut_100          	all	0.2953
ndcg_cut_200          	all	0.3265
ndcg_cut_500          	all	0.3688
ndcg_cut_1000         	all	0.3976
</pre>
 <h4>Miracl_EN_Run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.1837
ndcg_cut_10           	all	0.1965
ndcg_cut_15           	all	0.1783
ndcg_cut_20           	all	0.1666
ndcg_cut_30           	all	0.1503
ndcg_cut_100          	all	0.1565
ndcg_cut_200          	all	0.1811
ndcg_cut_500          	all	0.2112
ndcg_cut_1000         	all	0.2380
</pre>
 <h4>Miracl_EN_Run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.1592
ndcg_cut_10           	all	0.1833
ndcg_cut_15           	all	0.1643
ndcg_cut_20           	all	0.1573
ndcg_cut_30           	all	0.1465
ndcg_cut_100          	all	0.1676
ndcg_cut_200          	all	0.1896
ndcg_cut_500          	all	0.2220
ndcg_cut_1000         	all	0.2518
</pre>
 <h4>Miracl_EN_Run.4.dat</h4>
            <pre>
ndcg_cut_5            	all	0.1661
ndcg_cut_10           	all	0.1572
ndcg_cut_15           	all	0.1562
ndcg_cut_20           	all	0.1516
ndcg_cut_30           	all	0.1497
ndcg_cut_100          	all	0.1820
ndcg_cut_200          	all	0.2138
ndcg_cut_500          	all	0.2590
ndcg_cut_1000         	all	0.2937
</pre>
 <h4>Miracl_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2474
ndcg_cut_10           	all	0.2465
ndcg_cut_15           	all	0.2438
ndcg_cut_20           	all	0.2366
ndcg_cut_30           	all	0.2350
ndcg_cut_100          	all	0.2730
ndcg_cut_200          	all	0.3052
ndcg_cut_500          	all	0.3531
ndcg_cut_1000         	all	0.3823
</pre>

<h2>Readability-biased evaluation</h2>
        <p>For this year task, we have considered the factor of understandability of information (or readability) within the evaluation of the submissions, along with the topicality factor (normally referred to as (topical) relevance). Thus, along with (topical) relevance assessments (qrels), we have collected readability assessments (qread). These assessments were provided by judges along with the relevance assessments; however readability was assumed to be assessed independently of (topical) relevance. To account for understandability/readability in the evaluation, we have computed an understandability biased measure, uRBP, as defined in Zuccon&Koopman, <a href="http://zuccon.net/publications/medIR2014_healthsearch_readability.pdf">"Integrating understandability in the evaluation of consumer health search engines"</a>, MedIR 2014. We refer to that publication for the motivations and the details of the measure; note however that we did not use automated readability measures to estimate readability - we instead had actual readability assessments from the relevance assessors. Readability assessments were given on a 4 point scale (from 0 to 3): It is very technical and difficult to read and understand (label 0), It is somewhat technical and difficult to read and understand (label 1), It is somewhat easy to read and understand (label 2), It is very easy to read and understand (label 3).</p><p>The results below have been obtained with the binary relevance assessments (i.e. qrels.clef2015.test.bin.txt) and the graded readability assessments (i.e. i.e. qread.clef2015.test.graded.txt), and ubire-v0.1.0 as distributed on <a href="https://github.com/ielab/ubire">GitHub</a>.</br>The tool was ran as follows:</p>
<pre>java -jar /tools/ubire.0.1.jar --qrels-file=qrels/qrels.clef2015.test.bin.txt --qread-file=qrels/qread.clef2015.test.graded.txt --readability --rbp-p=0.8 --ranking-file=runName</pre></br><p>This computes RBP, and two versions of uRBP. The user persistence parameter <i>p</i> of RBP (and uRBP) was set to 0.8 following Park&Zhang, <a href="http://goanna.cs.rmit.edu.au/~aht/adcs2007/papers/17N.PDF">"On the distribution of user persistence for rank-biased precision"</a>, ADCS 2007.  uRBP has been computed by using user model 1 of Zuccon&Koopman with threshold=2, i.e. documents with a readability score of 0 or 1 where deemed not readable and thus had P(U|k)=0, while documents with a readability score of 2 or 3 where deemed readable and thus had P(U|k)=1. uRBPgr has been computed by mapping graded readability scores to different probability values, in particular: readability of 0 was assigned P(U|k)=0, readability of 1 was assigned P(U|k)=0.4, readability of 2 was assigned P(U|k)=0.8, readability of 3 was assigned P(U|k)=1.</br>Note that we are still experimenting with these readability-biased measures and thus observations made with the provided measures may not be conclusive.</p>
 <h4>Miracl_EN_Run.1.dat</h4>
            <pre>
RBP(0.8)	all      	0.3287
uRBP(0.8)	all      	0.2546
uRBPgr(0.8)	all      	0.2631
</pre>
 <h4>Miracl_EN_Run.2.dat</h4>
            <pre>
RBP(0.8)	all      	0.2291
uRBP(0.8)	all      	0.1589
uRBPgr(0.8)	all      	0.1626
</pre>
 <h4>Miracl_EN_Run.3.dat</h4>
            <pre>
RBP(0.8)	all      	0.2200
uRBP(0.8)	all      	0.1698
uRBPgr(0.8)	all      	0.1698
</pre>
 <h4>Miracl_EN_Run.4.dat</h4>
            <pre>
RBP(0.8)	all      	0.2001
uRBP(0.8)	all      	0.1507
uRBPgr(0.8)	all      	0.1570
</pre>
 <h4>Miracl_EN_Run.5.dat</h4>
            <pre>
RBP(0.8)	all      	0.2982
uRBP(0.8)	all      	0.2262
uRBPgr(0.8)	all      	0.2357
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar is then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>Miracl_EN_Run.1.dat</h4>
<img src="./img/Miracl_EN_Run.1.dat.p10.png"/>
 <h4>Miracl_EN_Run.2.dat</h4>
<img src="./img/Miracl_EN_Run.2.dat.p10.png"/>
 <h4>Miracl_EN_Run.3.dat</h4>
<img src="./img/Miracl_EN_Run.3.dat.p10.png"/>
 <h4>Miracl_EN_Run.4.dat</h4>
<img src="./img/Miracl_EN_Run.4.dat.p10.png"/>
 <h4>Miracl_EN_Run.5.dat</h4>
<img src="./img/Miracl_EN_Run.5.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/ielab/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
