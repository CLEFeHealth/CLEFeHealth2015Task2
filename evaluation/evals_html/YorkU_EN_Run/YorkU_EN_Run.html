<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2015 TASK 2 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2015 Task 2 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2015 TASK 2 Results - YorkU_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2015 TASK 2 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the two highest priority runs (run 2 and 3); thus all remaining runs were not sampled to form the assessment pool. This was because, unlike in previous years, submissions were greatly differing between each other, and thus lead to very large (and different) pools. As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page. Note that metrics such as MAP may not be reliable for evaluation due to the limited pool depth.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtest2015.XX id); others did not retrieve results for some queries or fail to conform to the TREC result format; finally, some retrieved the same document more than once for the same query (i.e. duplicate documents). To fairly evaluate the runs of every participant, submitted runs containing formatting errors and duplicated documents have been corrected. Duplicate documents have been removed and replaced by a <i>duplicatedid</i> string. Your runs (as evaluated here) are contained in the folder <i>runs</i> - please use these runs if you want to perform further evaluation and analysis.</br></br>Please refer to the <a href="https://sites.google.com/site/clefehealth2015/">ShARe/CLEF 2015 eHealth Evaluation Lab website</a> for additional details on the task.</p></br><p>Relevance assessments are distributed along with this webpage and are also available from the task website. Assessors judged relevance according to a three point scale: Not Relevant (label 0), Somewhat Relevant (label 1), Highly Relevant (label 2). When computing binary relevance measures (e.g. P@10 and MAP), we mapped label 0 to not relevant, and labels 1 and 2 to relevant; this  is encoded in the binary qrels named <i>qrels.clef2015.test.bin.txt</i>. Graded relevance assessments are contained in the <i>qrels.clef2015.test.graded.txt</i> file.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2015.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2015.test.bin.txt runName</pre>
 <h4>YorkU_EN_Run.1.dat</h4>
            <pre>
runid                 	all	BM25b0.0
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	995
map                   	all	0.0836
gm_map                	all	0.0172
Rprec                 	all	0.1071
bpref                 	all	0.1666
recip_rank            	all	0.2858
iprec_at_recall_0.00  	all	0.3329
iprec_at_recall_0.10  	all	0.2427
iprec_at_recall_0.20  	all	0.1489
iprec_at_recall_0.30  	all	0.1132
iprec_at_recall_0.40  	all	0.0706
iprec_at_recall_0.50  	all	0.0537
iprec_at_recall_0.60  	all	0.0443
iprec_at_recall_0.70  	all	0.0382
iprec_at_recall_0.80  	all	0.0222
iprec_at_recall_0.90  	all	0.0200
iprec_at_recall_1.00  	all	0.0200
P_5                   	all	0.1848
P_10                  	all	0.1894
P_15                  	all	0.1677
P_20                  	all	0.1455
P_30                  	all	0.1202
P_100                 	all	0.0655
P_200                 	all	0.0419
P_500                 	all	0.0230
P_1000                	all	0.0151
</pre>
 <h4>YorkU_EN_Run.10.dat</h4>
            <pre>
runid                 	all	BM25b0.9
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1038
map                   	all	0.1218
gm_map                	all	0.0271
Rprec                 	all	0.1529
bpref                 	all	0.2116
recip_rank            	all	0.4845
iprec_at_recall_0.00  	all	0.5154
iprec_at_recall_0.10  	all	0.3952
iprec_at_recall_0.20  	all	0.2734
iprec_at_recall_0.30  	all	0.1595
iprec_at_recall_0.40  	all	0.0994
iprec_at_recall_0.50  	all	0.0653
iprec_at_recall_0.60  	all	0.0324
iprec_at_recall_0.70  	all	0.0130
iprec_at_recall_0.80  	all	0.0064
iprec_at_recall_0.90  	all	0.0021
iprec_at_recall_1.00  	all	0.0021
P_5                   	all	0.2909
P_10                  	all	0.2667
P_15                  	all	0.2414
P_20                  	all	0.2167
P_30                  	all	0.1793
P_100                 	all	0.0830
P_200                 	all	0.0514
P_500                 	all	0.0275
P_1000                	all	0.0157
</pre>
 <h4>YorkU_EN_Run.2.dat</h4>
            <pre>
runid                 	all	BM25b0.31
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1082
map                   	all	0.1385
gm_map                	all	0.0385
Rprec                 	all	0.1745
bpref                 	all	0.2086
recip_rank            	all	0.5113
iprec_at_recall_0.00  	all	0.5490
iprec_at_recall_0.10  	all	0.4097
iprec_at_recall_0.20  	all	0.3065
iprec_at_recall_0.30  	all	0.2080
iprec_at_recall_0.40  	all	0.1147
iprec_at_recall_0.50  	all	0.0766
iprec_at_recall_0.60  	all	0.0397
iprec_at_recall_0.70  	all	0.0179
iprec_at_recall_0.80  	all	0.0098
iprec_at_recall_0.90  	all	0.0046
iprec_at_recall_1.00  	all	0.0046
P_5                   	all	0.3455
P_10                  	all	0.2924
P_15                  	all	0.2596
P_20                  	all	0.2265
P_30                  	all	0.1985
P_100                 	all	0.0964
P_200                 	all	0.0577
P_500                 	all	0.0290
P_1000                	all	0.0164
</pre>
 <h4>YorkU_EN_Run.3.dat</h4>
            <pre>
runid                 	all	BM25b0.2
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1078
map                   	all	0.1375
gm_map                	all	0.0376
Rprec                 	all	0.1638
bpref                 	all	0.1989
recip_rank            	all	0.5193
iprec_at_recall_0.00  	all	0.5587
iprec_at_recall_0.10  	all	0.3955
iprec_at_recall_0.20  	all	0.2816
iprec_at_recall_0.30  	all	0.2033
iprec_at_recall_0.40  	all	0.1203
iprec_at_recall_0.50  	all	0.0822
iprec_at_recall_0.60  	all	0.0414
iprec_at_recall_0.70  	all	0.0224
iprec_at_recall_0.80  	all	0.0135
iprec_at_recall_0.90  	all	0.0083
iprec_at_recall_1.00  	all	0.0083
P_5                   	all	0.3333
P_10                  	all	0.2803
P_15                  	all	0.2465
P_20                  	all	0.2152
P_30                  	all	0.1889
P_100                 	all	0.0970
P_200                 	all	0.0584
P_500                 	all	0.0286
P_1000                	all	0.0163
</pre>
 <h4>YorkU_EN_Run.4.dat</h4>
            <pre>
runid                 	all	BM25b0.3
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1079
map                   	all	0.1386
gm_map                	all	0.0387
Rprec                 	all	0.1745
bpref                 	all	0.2081
recip_rank            	all	0.5150
iprec_at_recall_0.00  	all	0.5534
iprec_at_recall_0.10  	all	0.4094
iprec_at_recall_0.20  	all	0.3053
iprec_at_recall_0.30  	all	0.2085
iprec_at_recall_0.40  	all	0.1144
iprec_at_recall_0.50  	all	0.0765
iprec_at_recall_0.60  	all	0.0395
iprec_at_recall_0.70  	all	0.0179
iprec_at_recall_0.80  	all	0.0097
iprec_at_recall_0.90  	all	0.0045
iprec_at_recall_1.00  	all	0.0045
P_5                   	all	0.3455
P_10                  	all	0.2924
P_15                  	all	0.2616
P_20                  	all	0.2265
P_30                  	all	0.1985
P_100                 	all	0.0967
P_200                 	all	0.0579
P_500                 	all	0.0290
P_1000                	all	0.0163
</pre>
 <h4>YorkU_EN_Run.5.dat</h4>
            <pre>
runid                 	all	BM25b0.4
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1075
map                   	all	0.1372
gm_map                	all	0.0366
Rprec                 	all	0.1752
bpref                 	all	0.2127
recip_rank            	all	0.5031
iprec_at_recall_0.00  	all	0.5428
iprec_at_recall_0.10  	all	0.4003
iprec_at_recall_0.20  	all	0.3157
iprec_at_recall_0.30  	all	0.2117
iprec_at_recall_0.40  	all	0.1117
iprec_at_recall_0.50  	all	0.0782
iprec_at_recall_0.60  	all	0.0430
iprec_at_recall_0.70  	all	0.0168
iprec_at_recall_0.80  	all	0.0086
iprec_at_recall_0.90  	all	0.0034
iprec_at_recall_1.00  	all	0.0034
P_5                   	all	0.3333
P_10                  	all	0.3000
P_15                  	all	0.2596
P_20                  	all	0.2326
P_30                  	all	0.2000
P_100                 	all	0.0956
P_200                 	all	0.0576
P_500                 	all	0.0290
P_1000                	all	0.0163
</pre>
 <h4>YorkU_EN_Run.6.dat</h4>
            <pre>
runid                 	all	BM25b0.5
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1072
map                   	all	0.1352
gm_map                	all	0.0345
Rprec                 	all	0.1703
bpref                 	all	0.2140
recip_rank            	all	0.4857
iprec_at_recall_0.00  	all	0.5253
iprec_at_recall_0.10  	all	0.4139
iprec_at_recall_0.20  	all	0.3237
iprec_at_recall_0.30  	all	0.2048
iprec_at_recall_0.40  	all	0.1097
iprec_at_recall_0.50  	all	0.0770
iprec_at_recall_0.60  	all	0.0418
iprec_at_recall_0.70  	all	0.0162
iprec_at_recall_0.80  	all	0.0091
iprec_at_recall_0.90  	all	0.0030
iprec_at_recall_1.00  	all	0.0030
P_5                   	all	0.3303
P_10                  	all	0.2924
P_15                  	all	0.2667
P_20                  	all	0.2439
P_30                  	all	0.2056
P_100                 	all	0.0936
P_200                 	all	0.0561
P_500                 	all	0.0290
P_1000                	all	0.0162
</pre>
 <h4>YorkU_EN_Run.7.dat</h4>
            <pre>
runid                 	all	BM25b0.6
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1070
map                   	all	0.1347
gm_map                	all	0.0334
Rprec                 	all	0.1671
bpref                 	all	0.2159
recip_rank            	all	0.4893
iprec_at_recall_0.00  	all	0.5305
iprec_at_recall_0.10  	all	0.4296
iprec_at_recall_0.20  	all	0.3225
iprec_at_recall_0.30  	all	0.1968
iprec_at_recall_0.40  	all	0.1087
iprec_at_recall_0.50  	all	0.0762
iprec_at_recall_0.60  	all	0.0387
iprec_at_recall_0.70  	all	0.0155
iprec_at_recall_0.80  	all	0.0088
iprec_at_recall_0.90  	all	0.0028
iprec_at_recall_1.00  	all	0.0028
P_5                   	all	0.3394
P_10                  	all	0.3015
P_15                  	all	0.2626
P_20                  	all	0.2417
P_30                  	all	0.2000
P_100                 	all	0.0908
P_200                 	all	0.0548
P_500                 	all	0.0285
P_1000                	all	0.0162
</pre>
 <h4>YorkU_EN_Run.8.dat</h4>
            <pre>
runid                 	all	BM25b0.7
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1065
map                   	all	0.1320
gm_map                	all	0.0341
Rprec                 	all	0.1640
bpref                 	all	0.2154
recip_rank            	all	0.4886
iprec_at_recall_0.00  	all	0.5298
iprec_at_recall_0.10  	all	0.4258
iprec_at_recall_0.20  	all	0.3049
iprec_at_recall_0.30  	all	0.1889
iprec_at_recall_0.40  	all	0.1062
iprec_at_recall_0.50  	all	0.0745
iprec_at_recall_0.60  	all	0.0372
iprec_at_recall_0.70  	all	0.0145
iprec_at_recall_0.80  	all	0.0086
iprec_at_recall_0.90  	all	0.0027
iprec_at_recall_1.00  	all	0.0027
P_5                   	all	0.3212
P_10                  	all	0.2939
P_15                  	all	0.2606
P_20                  	all	0.2356
P_30                  	all	0.1944
P_100                 	all	0.0891
P_200                 	all	0.0538
P_500                 	all	0.0282
P_1000                	all	0.0161
</pre>
 <h4>YorkU_EN_Run.9.dat</h4>
            <pre>
runid                 	all	BM25b0.8
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1059
map                   	all	0.1279
gm_map                	all	0.0323
Rprec                 	all	0.1639
bpref                 	all	0.2147
recip_rank            	all	0.4899
iprec_at_recall_0.00  	all	0.5245
iprec_at_recall_0.10  	all	0.4102
iprec_at_recall_0.20  	all	0.2983
iprec_at_recall_0.30  	all	0.1681
iprec_at_recall_0.40  	all	0.1031
iprec_at_recall_0.50  	all	0.0733
iprec_at_recall_0.60  	all	0.0366
iprec_at_recall_0.70  	all	0.0130
iprec_at_recall_0.80  	all	0.0083
iprec_at_recall_0.90  	all	0.0025
iprec_at_recall_1.00  	all	0.0025
P_5                   	all	0.3061
P_10                  	all	0.2788
P_15                  	all	0.2586
P_20                  	all	0.2341
P_30                  	all	0.1874
P_100                 	all	0.0876
P_200                 	all	0.0533
P_500                 	all	0.0281
P_1000                	all	0.0160
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2015.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2015.test.graded.txt runName</pre></br><p>This computes nDCG according to Jarvelin and Kekalainen (ACM ToIS v. 20, pp. 422-446, 2002). Gain values are the relevance values in the qrels file (i.e. label 0 corresponds to gain 0, label 1 to gain 1 and label 2 to gain 2).</p>
 <h4>YorkU_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.1538
ndcg_cut_10           	all	0.1718
ndcg_cut_15           	all	0.1708
ndcg_cut_20           	all	0.1623
ndcg_cut_30           	all	0.1576
ndcg_cut_100          	all	0.1708
ndcg_cut_200          	all	0.1956
ndcg_cut_500          	all	0.2251
ndcg_cut_1000         	all	0.2512
</pre>
 <h4>YorkU_EN_Run.10.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2610
ndcg_cut_10           	all	0.2546
ndcg_cut_15           	all	0.2451
ndcg_cut_20           	all	0.2381
ndcg_cut_30           	all	0.2254
ndcg_cut_100          	all	0.2295
ndcg_cut_200          	all	0.2527
ndcg_cut_500          	all	0.2870
ndcg_cut_1000         	all	0.3022
</pre>
 <h4>YorkU_EN_Run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2890
ndcg_cut_10           	all	0.2714
ndcg_cut_15           	all	0.2660
ndcg_cut_20           	all	0.2537
ndcg_cut_30           	all	0.2526
ndcg_cut_100          	all	0.2641
ndcg_cut_200          	all	0.2889
ndcg_cut_500          	all	0.3146
ndcg_cut_1000         	all	0.3287
</pre>
 <h4>YorkU_EN_Run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2937
ndcg_cut_10           	all	0.2719
ndcg_cut_15           	all	0.2601
ndcg_cut_20           	all	0.2502
ndcg_cut_30           	all	0.2543
ndcg_cut_100          	all	0.2683
ndcg_cut_200          	all	0.2917
ndcg_cut_500          	all	0.3145
ndcg_cut_1000         	all	0.3291
</pre>
 <h4>YorkU_EN_Run.4.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2891
ndcg_cut_10           	all	0.2717
ndcg_cut_15           	all	0.2674
ndcg_cut_20           	all	0.2539
ndcg_cut_30           	all	0.2526
ndcg_cut_100          	all	0.2645
ndcg_cut_200          	all	0.2893
ndcg_cut_500          	all	0.3148
ndcg_cut_1000         	all	0.3288
</pre>
 <h4>YorkU_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2808
ndcg_cut_10           	all	0.2752
ndcg_cut_15           	all	0.2621
ndcg_cut_20           	all	0.2529
ndcg_cut_30           	all	0.2477
ndcg_cut_100          	all	0.2596
ndcg_cut_200          	all	0.2809
ndcg_cut_500          	all	0.3102
ndcg_cut_1000         	all	0.3238
</pre>
 <h4>YorkU_EN_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2797
ndcg_cut_10           	all	0.2694
ndcg_cut_15           	all	0.2655
ndcg_cut_20           	all	0.2576
ndcg_cut_30           	all	0.2503
ndcg_cut_100          	all	0.2531
ndcg_cut_200          	all	0.2746
ndcg_cut_500          	all	0.3053
ndcg_cut_1000         	all	0.3196
</pre>
 <h4>YorkU_EN_Run.7.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2887
ndcg_cut_10           	all	0.2766
ndcg_cut_15           	all	0.2655
ndcg_cut_20           	all	0.2584
ndcg_cut_30           	all	0.2477
ndcg_cut_100          	all	0.2514
ndcg_cut_200          	all	0.2731
ndcg_cut_500          	all	0.3040
ndcg_cut_1000         	all	0.3181
</pre>
 <h4>YorkU_EN_Run.8.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2760
ndcg_cut_10           	all	0.2729
ndcg_cut_15           	all	0.2641
ndcg_cut_20           	all	0.2546
ndcg_cut_30           	all	0.2445
ndcg_cut_100          	all	0.2466
ndcg_cut_200          	all	0.2675
ndcg_cut_500          	all	0.2991
ndcg_cut_1000         	all	0.3159
</pre>
 <h4>YorkU_EN_Run.9.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2710
ndcg_cut_10           	all	0.2637
ndcg_cut_15           	all	0.2605
ndcg_cut_20           	all	0.2513
ndcg_cut_30           	all	0.2335
ndcg_cut_100          	all	0.2414
ndcg_cut_200          	all	0.2638
ndcg_cut_500          	all	0.2972
ndcg_cut_1000         	all	0.3127
</pre>

<h2>Readability-biased evaluation</h2>
        <p>For this year task, we have considered the factor of understandability of information (or readability) within the evaluation of the submissions, along with the topicality factor (normally referred to as (topical) relevance). Thus, along with (topical) relevance assessments (qrels), we have collected readability assessments (qread). These assessments were provided by judges along with the relevance assessments; however readability was assumed to be assessed independently of (topical) relevance. To account for understandability/readability in the evaluation, we have computed an understandability biased measure, uRBP, as defined in Zuccon&Koopman, <a href="http://zuccon.net/publications/medIR2014_healthsearch_readability.pdf">"Integrating understandability in the evaluation of consumer health search engines"</a>, MedIR 2014. We refer to that publication for the motivations and the details of the measure; note however that we did not use automated readability measures to estimate readability - we instead had actual readability assessments from the relevance assessors. Readability assessments were given on a 4 point scale (from 0 to 3): It is very technical and difficult to read and understand (label 0), It is somewhat technical and difficult to read and understand (label 1), It is somewhat easy to read and understand (label 2), It is very easy to read and understand (label 3).</p><p>The results below have been obtained with the binary relevance assessments (i.e. qrels.clef2015.test.bin.txt) and the graded readability assessments (i.e. i.e. qread.clef2015.test.graded.txt), and ubire-v0.1.0 as distributed on <a href="https://github.com/ielab/ubire">GitHub</a>.</br>The tool was ran as follows:</p>
<pre>java -jar /tools/ubire.0.1.jar --qrels-file=qrels/qrels.clef2015.test.bin.txt --qread-file=qrels/qread.clef2015.test.graded.txt --readability --rbp-p=0.8 --ranking-file=runName</pre></br><p>This computes RBP, and two versions of uRBP. The user persistence parameter <i>p</i> of RBP (and uRBP) was set to 0.8 following Park&Zhang, <a href="http://goanna.cs.rmit.edu.au/~aht/adcs2007/papers/17N.PDF">"On the distribution of user persistence for rank-biased precision"</a>, ADCS 2007.  uRBP has been computed by using user model 1 of Zuccon&Koopman with threshold=2, i.e. documents with a readability score of 0 or 1 where deemed not readable and thus had P(U|k)=0, while documents with a readability score of 2 or 3 where deemed readable and thus had P(U|k)=1. uRBPgr has been computed by mapping graded readability scores to different probability values, in particular: readability of 0 was assigned P(U|k)=0, readability of 1 was assigned P(U|k)=0.4, readability of 2 was assigned P(U|k)=0.8, readability of 3 was assigned P(U|k)=1.</br>Note that we are still experimenting with these readability-biased measures and thus observations made with the provided measures may not be conclusive.</p>
 <h4>YorkU_EN_Run.1.dat</h4>
            <pre>
RBP(0.8)	all      	0.1798
uRBP(0.8)	all      	0.1127
uRBPgr(0.8)	all      	0.1195
</pre>
 <h4>YorkU_EN_Run.10.dat</h4>
            <pre>
RBP(0.8)	all      	0.2853
uRBP(0.8)	all      	0.2415
uRBPgr(0.8)	all      	0.2420
</pre>
 <h4>YorkU_EN_Run.2.dat</h4>
            <pre>
RBP(0.8)	all      	0.3151
uRBP(0.8)	all      	0.2334
uRBPgr(0.8)	all      	0.2404
</pre>
 <h4>YorkU_EN_Run.3.dat</h4>
            <pre>
RBP(0.8)	all      	0.3074
uRBP(0.8)	all      	0.2216
uRBPgr(0.8)	all      	0.2300
</pre>
 <h4>YorkU_EN_Run.4.dat</h4>
            <pre>
RBP(0.8)	all      	0.3152
uRBP(0.8)	all      	0.2319
uRBPgr(0.8)	all      	0.2397
</pre>
 <h4>YorkU_EN_Run.5.dat</h4>
            <pre>
RBP(0.8)	all      	0.3109
uRBP(0.8)	all      	0.2357
uRBPgr(0.8)	all      	0.2416
</pre>
 <h4>YorkU_EN_Run.6.dat</h4>
            <pre>
RBP(0.8)	all      	0.3081
uRBP(0.8)	all      	0.2365
uRBPgr(0.8)	all      	0.2431
</pre>
 <h4>YorkU_EN_Run.7.dat</h4>
            <pre>
RBP(0.8)	all      	0.3125
uRBP(0.8)	all      	0.2470
uRBPgr(0.8)	all      	0.2523
</pre>
 <h4>YorkU_EN_Run.8.dat</h4>
            <pre>
RBP(0.8)	all      	0.3072
uRBP(0.8)	all      	0.2504
uRBPgr(0.8)	all      	0.2533
</pre>
 <h4>YorkU_EN_Run.9.dat</h4>
            <pre>
RBP(0.8)	all      	0.2962
uRBP(0.8)	all      	0.2470
uRBPgr(0.8)	all      	0.2485
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar is then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>YorkU_EN_Run.1.dat</h4>
<img src="./img/YorkU_EN_Run.1.dat.p10.png"/>
 <h4>YorkU_EN_Run.10.dat</h4>
<img src="./img/YorkU_EN_Run.10.dat.p10.png"/>
 <h4>YorkU_EN_Run.2.dat</h4>
<img src="./img/YorkU_EN_Run.2.dat.p10.png"/>
 <h4>YorkU_EN_Run.3.dat</h4>
<img src="./img/YorkU_EN_Run.3.dat.p10.png"/>
 <h4>YorkU_EN_Run.4.dat</h4>
<img src="./img/YorkU_EN_Run.4.dat.p10.png"/>
 <h4>YorkU_EN_Run.5.dat</h4>
<img src="./img/YorkU_EN_Run.5.dat.p10.png"/>
 <h4>YorkU_EN_Run.6.dat</h4>
<img src="./img/YorkU_EN_Run.6.dat.p10.png"/>
 <h4>YorkU_EN_Run.7.dat</h4>
<img src="./img/YorkU_EN_Run.7.dat.p10.png"/>
 <h4>YorkU_EN_Run.8.dat</h4>
<img src="./img/YorkU_EN_Run.8.dat.p10.png"/>
 <h4>YorkU_EN_Run.9.dat</h4>
<img src="./img/YorkU_EN_Run.9.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/ielab/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
