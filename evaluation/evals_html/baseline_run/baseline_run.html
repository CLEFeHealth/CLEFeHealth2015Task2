<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2015 TASK 2 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2015 Task 2 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2015 TASK 2 Results - baseline_run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2015 TASK 2 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the two highest priority runs (run 2 and 3); thus all remaining runs were not sampled to form the assessment pool. This was because, unlike in previous years, submissions were greatly differing between each other, and thus lead to very large (and different) pools. As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page. Note that metrics such as MAP may not be reliable for evaluation due to the limited pool depth.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtest2015.XX id); others did not retrieve results for some queries or fail to conform to the TREC result format; finally, some retrieved the same document more than once for the same query (i.e. duplicate documents). To fairly evaluate the runs of every participant, submitted runs containing formatting errors and duplicated documents have been corrected. Duplicate documents have been removed and replaced by a <i>duplicatedid</i> string. Your runs (as evaluated here) are contained in the folder <i>runs</i> - please use these runs if you want to perform further evaluation and analysis.</br></br>Please refer to the <a href="https://sites.google.com/site/clefehealth2015/">ShARe/CLEF 2015 eHealth Evaluation Lab website</a> for additional details on the task.</p></br><p>Relevance assessments are distributed along with this webpage and are also available from the task website. Assessors judged relevance according to a three point scale: Not Relevant (label 0), Somewhat Relevant (label 1), Highly Relevant (label 2). When computing binary relevance measures (e.g. P@10 and MAP), we mapped label 0 to not relevant, and labels 1 and 2 to relevant; this  is encoded in the binary qrels named <i>qrels.clef2015.test.bin.txt</i>. Graded relevance assessments are contained in the <i>qrels.clef2015.test.graded.txt</i> file.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2015.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2015.test.bin.txt runName</pre>
 <h4>baseline_run.1.dat</h4>
            <pre>
runid                 	all	Xapian_BM25_bs4
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1594
map                   	all	0.2083
gm_map                	all	0.0634
Rprec                 	all	0.2316
bpref                 	all	0.2833
recip_rank            	all	0.5608
iprec_at_recall_0.00  	all	0.5998
iprec_at_recall_0.10  	all	0.4549
iprec_at_recall_0.20  	all	0.3462
iprec_at_recall_0.30  	all	0.3009
iprec_at_recall_0.40  	all	0.2462
iprec_at_recall_0.50  	all	0.1980
iprec_at_recall_0.60  	all	0.1330
iprec_at_recall_0.70  	all	0.0983
iprec_at_recall_0.80  	all	0.0691
iprec_at_recall_0.90  	all	0.0324
iprec_at_recall_1.00  	all	0.0180
P_5                   	all	0.3879
P_10                  	all	0.3333
P_15                  	all	0.3040
P_20                  	all	0.2765
P_30                  	all	0.2414
P_100                 	all	0.1321
P_200                 	all	0.0872
P_500                 	all	0.0436
P_1000                	all	0.0242
</pre>
 <h4>baseline_run.2.dat</h4>
            <pre>
runid                 	all	lucene_html_vsm
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1521
map                   	all	0.1473
gm_map                	all	0.0512
Rprec                 	all	0.1701
bpref                 	all	0.2500
recip_rank            	all	0.4330
iprec_at_recall_0.00  	all	0.4892
iprec_at_recall_0.10  	all	0.3907
iprec_at_recall_0.20  	all	0.2768
iprec_at_recall_0.30  	all	0.1990
iprec_at_recall_0.40  	all	0.1541
iprec_at_recall_0.50  	all	0.1187
iprec_at_recall_0.60  	all	0.0903
iprec_at_recall_0.70  	all	0.0603
iprec_at_recall_0.80  	all	0.0386
iprec_at_recall_0.90  	all	0.0153
iprec_at_recall_1.00  	all	0.0043
P_5                   	all	0.3424
P_10                  	all	0.3015
P_15                  	all	0.2616
P_20                  	all	0.2333
P_30                  	all	0.2030
P_100                 	all	0.1056
P_200                 	all	0.0701
P_500                 	all	0.0391
P_1000                	all	0.0230
</pre>
 <h4>baseline_run.3.dat</h4>
            <pre>
runid                 	all	lucene_bs4_lm
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1509
map                   	all	0.2098
gm_map                	all	0.0802
Rprec                 	all	0.2426
bpref                 	all	0.2813
recip_rank            	all	0.5226
iprec_at_recall_0.00  	all	0.5850
iprec_at_recall_0.10  	all	0.4855
iprec_at_recall_0.20  	all	0.3752
iprec_at_recall_0.30  	all	0.3151
iprec_at_recall_0.40  	all	0.2461
iprec_at_recall_0.50  	all	0.1855
iprec_at_recall_0.60  	all	0.1534
iprec_at_recall_0.70  	all	0.0987
iprec_at_recall_0.80  	all	0.0631
iprec_at_recall_0.90  	all	0.0353
iprec_at_recall_1.00  	all	0.0216
P_5                   	all	0.3394
P_10                  	all	0.3242
P_15                  	all	0.3030
P_20                  	all	0.2841
P_30                  	all	0.2505
P_100                 	all	0.1344
P_200                 	all	0.0842
P_500                 	all	0.0407
P_1000                	all	0.0229
</pre>
 <h4>baseline_run.4.dat</h4>
            <pre>
runid                 	all	Xapian_BM25_bs4_disease
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1452
map                   	all	0.2140
gm_map                	all	0.0685
Rprec                 	all	0.2170
bpref                 	all	0.5153
recip_rank            	all	0.5207
iprec_at_recall_0.00  	all	0.5619
iprec_at_recall_0.10  	all	0.4291
iprec_at_recall_0.20  	all	0.3543
iprec_at_recall_0.30  	all	0.2893
iprec_at_recall_0.40  	all	0.2349
iprec_at_recall_0.50  	all	0.2075
iprec_at_recall_0.60  	all	0.1473
iprec_at_recall_0.70  	all	0.1193
iprec_at_recall_0.80  	all	0.1013
iprec_at_recall_0.90  	all	0.0756
iprec_at_recall_1.00  	all	0.0614
P_5                   	all	0.3667
P_10                  	all	0.2848
P_15                  	all	0.2354
P_20                  	all	0.2023
P_30                  	all	0.1662
P_100                 	all	0.0862
P_200                 	all	0.0567
P_500                 	all	0.0351
P_1000                	all	0.0220
</pre>
 <h4>baseline_run.5.dat</h4>
            <pre>
runid                 	all	lucene_html_vsm_disease
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1426
map                   	all	0.1372
gm_map                	all	0.0428
Rprec                 	all	0.1457
bpref                 	all	0.5099
recip_rank            	all	0.4220
iprec_at_recall_0.00  	all	0.4506
iprec_at_recall_0.10  	all	0.3039
iprec_at_recall_0.20  	all	0.2334
iprec_at_recall_0.30  	all	0.1837
iprec_at_recall_0.40  	all	0.1474
iprec_at_recall_0.50  	all	0.1239
iprec_at_recall_0.60  	all	0.0996
iprec_at_recall_0.70  	all	0.0747
iprec_at_recall_0.80  	all	0.0517
iprec_at_recall_0.90  	all	0.0304
iprec_at_recall_1.00  	all	0.0252
P_5                   	all	0.2333
P_10                  	all	0.1955
P_15                  	all	0.1717
P_20                  	all	0.1538
P_30                  	all	0.1328
P_100                 	all	0.0791
P_200                 	all	0.0561
P_500                 	all	0.0338
P_1000                	all	0.0216
</pre>
 <h4>baseline_run.6.dat</h4>
            <pre>
runid                 	all	lucene_bs4_lm_disease
num_q                 	all	66
num_ret               	all	66000
num_rel               	all	1972
num_rel_ret           	all	1427
map                   	all	0.1788
gm_map                	all	0.0674
Rprec                 	all	0.1901
bpref                 	all	0.5022
recip_rank            	all	0.4793
iprec_at_recall_0.00  	all	0.5237
iprec_at_recall_0.10  	all	0.3763
iprec_at_recall_0.20  	all	0.2955
iprec_at_recall_0.30  	all	0.2383
iprec_at_recall_0.40  	all	0.1849
iprec_at_recall_0.50  	all	0.1562
iprec_at_recall_0.60  	all	0.1231
iprec_at_recall_0.70  	all	0.1066
iprec_at_recall_0.80  	all	0.0889
iprec_at_recall_0.90  	all	0.0508
iprec_at_recall_1.00  	all	0.0445
P_5                   	all	0.3121
P_10                  	all	0.2621
P_15                  	all	0.2172
P_20                  	all	0.2068
P_30                  	all	0.1758
P_100                 	all	0.0877
P_200                 	all	0.0609
P_500                 	all	0.0355
P_1000                	all	0.0216
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2015.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2015.test.graded.txt runName</pre></br><p>This computes nDCG according to Jarvelin and Kekalainen (ACM ToIS v. 20, pp. 422-446, 2002). Gain values are the relevance values in the qrels file (i.e. label 0 corresponds to gain 0, label 1 to gain 1 and label 2 to gain 2).</p>
 <h4>baseline_run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3301
ndcg_cut_10           	all	0.3151
ndcg_cut_15           	all	0.3060
ndcg_cut_20           	all	0.3004
ndcg_cut_30           	all	0.2961
ndcg_cut_100          	all	0.3345
ndcg_cut_200          	all	0.3862
ndcg_cut_500          	all	0.4265
ndcg_cut_1000         	all	0.4503
</pre>
 <h4>baseline_run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2455
ndcg_cut_10           	all	0.2479
ndcg_cut_15           	all	0.2415
ndcg_cut_20           	all	0.2344
ndcg_cut_30           	all	0.2348
ndcg_cut_100          	all	0.2671
ndcg_cut_200          	all	0.3091
ndcg_cut_500          	all	0.3642
ndcg_cut_1000         	all	0.3929
</pre>
 <h4>baseline_run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2808
ndcg_cut_10           	all	0.2960
ndcg_cut_15           	all	0.3017
ndcg_cut_20           	all	0.3003
ndcg_cut_30           	all	0.3080
ndcg_cut_100          	all	0.3439
ndcg_cut_200          	all	0.3858
ndcg_cut_500          	all	0.4236
ndcg_cut_1000         	all	0.4474
</pre>
 <h4>baseline_run.4.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3682
ndcg_cut_10           	all	0.3483
ndcg_cut_15           	all	0.3332
ndcg_cut_20           	all	0.3215
ndcg_cut_30           	all	0.3184
ndcg_cut_100          	all	0.3441
ndcg_cut_200          	all	0.3745
ndcg_cut_500          	all	0.4321
ndcg_cut_1000         	all	0.4682
</pre>
 <h4>baseline_run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.2511
ndcg_cut_10           	all	0.2417
ndcg_cut_15           	all	0.2346
ndcg_cut_20           	all	0.2303
ndcg_cut_30           	all	0.2313
ndcg_cut_100          	all	0.2744
ndcg_cut_200          	all	0.3130
ndcg_cut_500          	all	0.3636
ndcg_cut_1000         	all	0.4041
</pre>
 <h4>baseline_run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3174
ndcg_cut_10           	all	0.3123
ndcg_cut_15           	all	0.2995
ndcg_cut_20           	all	0.3020
ndcg_cut_30           	all	0.3042
ndcg_cut_100          	all	0.3224
ndcg_cut_200          	all	0.3616
ndcg_cut_500          	all	0.4116
ndcg_cut_1000         	all	0.4425
</pre>

<h2>Readability-biased evaluation</h2>
        <p>For this year task, we have considered the factor of understandability of information (or readability) within the evaluation of the submissions, along with the topicality factor (normally referred to as (topical) relevance). Thus, along with (topical) relevance assessments (qrels), we have collected readability assessments (qread). These assessments were provided by judges along with the relevance assessments; however readability was assumed to be assessed independently of (topical) relevance. To account for understandability/readability in the evaluation, we have computed an understandability biased measure, uRBP, as defined in Zuccon&Koopman, <a href="http://zuccon.net/publications/medIR2014_healthsearch_readability.pdf">"Integrating understandability in the evaluation of consumer health search engines"</a>, MedIR 2014. We refer to that publication for the motivations and the details of the measure; note however that we did not use automated readability measures to estimate readability - we instead had actual readability assessments from the relevance assessors. Readability assessments were given on a 4 point scale (from 0 to 3): It is very technical and difficult to read and understand (label 0), It is somewhat technical and difficult to read and understand (label 1), It is somewhat easy to read and understand (label 2), It is very easy to read and understand (label 3).</p><p>The results below have been obtained with the binary relevance assessments (i.e. qrels.clef2015.test.bin.txt) and the graded readability assessments (i.e. i.e. qread.clef2015.test.graded.txt), and ubire-v0.1.0 as distributed on <a href="https://github.com/ielab/ubire">GitHub</a>.</br>The tool was ran as follows:</p>
<pre>java -jar /tools/ubire.0.1.jar --qrels-file=qrels/qrels.clef2015.test.bin.txt --qread-file=qrels/qread.clef2015.test.graded.txt --readability --rbp-p=0.8 --ranking-file=runName</pre></br><p>This computes RBP, and two versions of uRBP. The user persistence parameter <i>p</i> of RBP (and uRBP) was set to 0.8 following Park&Zhang, <a href="http://goanna.cs.rmit.edu.au/~aht/adcs2007/papers/17N.PDF">"On the distribution of user persistence for rank-biased precision"</a>, ADCS 2007.  uRBP has been computed by using user model 1 of Zuccon&Koopman with threshold=2, i.e. documents with a readability score of 0 or 1 where deemed not readable and thus had P(U|k)=0, while documents with a readability score of 2 or 3 where deemed readable and thus had P(U|k)=1. uRBPgr has been computed by mapping graded readability scores to different probability values, in particular: readability of 0 was assigned P(U|k)=0, readability of 1 was assigned P(U|k)=0.4, readability of 2 was assigned P(U|k)=0.8, readability of 3 was assigned P(U|k)=1.</br>Note that we are still experimenting with these readability-biased measures and thus observations made with the provided measures may not be conclusive.</p>
 <h4>baseline_run.1.dat</h4>
            <pre>
RBP(0.8)	all      	0.3567
uRBP(0.8)	all      	0.2990
uRBPgr(0.8)	all      	0.2933
</pre>
 <h4>baseline_run.2.dat</h4>
            <pre>
RBP(0.8)	all      	0.3150
uRBP(0.8)	all      	0.2633
uRBPgr(0.8)	all      	0.2587
</pre>
 <h4>baseline_run.3.dat</h4>
            <pre>
RBP(0.8)	all      	0.3369
uRBP(0.8)	all      	0.2736
uRBPgr(0.8)	all      	0.2751
</pre>
 <h4>baseline_run.4.dat</h4>
            <pre>
RBP(0.8)	all      	0.3196
uRBP(0.8)	all      	0.2291
uRBPgr(0.8)	all      	0.2323
</pre>
 <h4>baseline_run.5.dat</h4>
            <pre>
RBP(0.8)	all      	0.2226
uRBP(0.8)	all      	0.1530
uRBPgr(0.8)	all      	0.1610
</pre>
 <h4>baseline_run.6.dat</h4>
            <pre>
RBP(0.8)	all      	0.2843
uRBP(0.8)	all      	0.2035
uRBPgr(0.8)	all      	0.2143
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar is then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>baseline_run.1.dat</h4>
<img src="./img/baseline_run.1.dat.p10.png"/>
 <h4>baseline_run.2.dat</h4>
<img src="./img/baseline_run.2.dat.p10.png"/>
 <h4>baseline_run.3.dat</h4>
<img src="./img/baseline_run.3.dat.p10.png"/>
 <h4>baseline_run.4.dat</h4>
<img src="./img/baseline_run.4.dat.p10.png"/>
 <h4>baseline_run.5.dat</h4>
<img src="./img/baseline_run.5.dat.p10.png"/>
 <h4>baseline_run.6.dat</h4>
<img src="./img/baseline_run.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/ielab/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
